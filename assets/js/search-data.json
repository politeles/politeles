{
  
    
        "post0": {
            "title": " reusing docker desktop",
            "content": "Reusing docker desktop . After some time doing experiments on kubernetes, I want to work in docker for windows using kubernetes. Docker for desktop documentation. . Open a terminal . PS C: Users polit&gt; kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE docker-desktop docker-desktop docker-desktop * dppizza dppizza dppizza default . and change the current to point to docker-desktop. . PS C: Users polit&gt; kubectl config use-context docker-desktop Switched to context &quot;docker-desktop&quot;. .",
            "url": "https://jpons.es/2023/05/25/Reusing-docker-desktop",
            "relUrl": "/2023/05/25/Reusing-docker-desktop",
            "date": " • May 25, 2023"
        }
        
    
  
    
        ,"post1": {
            "title": "Handle odd elements with Hugo",
            "content": "Handle odd elements with Hugo . We want to attach a CSS property to some objects . &lt;div class=&quot;col-md-4 align-self-center wwm-down&quot;&gt; &lt;/div&gt; . (https://discourse.gohugo.io/t/detect-every-odd-post-in-a-range/6582)[More info here] . Not loading elements in the parent folder . Inside the parent pages folder we should have a _index.md file . (https://github.com/gohugoio/hugo/issues/7362)[Github issue] .",
            "url": "https://jpons.es/2023/02/27/Hugo-handle-odd",
            "relUrl": "/2023/02/27/Hugo-handle-odd",
            "date": " • Feb 27, 2023"
        }
        
    
  
    
        ,"post2": {
            "title": "Kubernetes cheat sheet",
            "content": "Kubernetes cheat sheet . Get nodes . kubectl get node kubectl get no . With more outuput: kubectl get nodes -o wide kubectl get no -o yaml . With a JSON Query kubectl get nodes -o json | jq “.items[] | {name:.metadata.name} + .status.capacity” . Exploring types and definitions . kubectl explain node kubectl explain node.spec . kubectl explain node –recursive . Viewing details . kubectl describe node . List of running pods . kubectl get pods . Namespaces . kubectl get namespaces . To get all namespaces . kubectl get pods –all-namespaces kubectl get pods -A . Scoping another namespaces . kubectl get pods -n kube-system . Creation / deletion of namespaces . kubectl create -n=X kubectl delete -n=X To add / remove and update labels across multiple namespaces: kubectl label . Kube-public . To get the cluster info: kubectl -n kube-public get configmaps . kubectl -n kube-public get configmap cluster-info -o yaml . Services . A service is an endpoint . kubectl get services kubectl get svc . Example output: PS C: Users polit sources container.training&gt; kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 443/TCP 15h . We can connect to the API using the CLUSTER-IP value. . ClusterIP services . Connections to the CLUSTER-IP can be done only from within the cluster. . DNS integration . Each service gets a DNS record The kubernetes DNS resolver is available from within the pods.(and sometimes from nodes depending on config). . Starting a pod . kubectl run . kubectl run pingpong –image alpine ping 127.0.0.1 . viewing logs . unless specified it will show the logs of the first container only . kubectl logs pingpong . kubectl logs pingpong –tail 1 -follow . -f/–follow to stream logs in real time –tail to indicate how many lines you want to see (from the end) –since to get logs only after a given timestamp . Creating a deployment . kubectl create deployment pingpong –image=alpine – ping 127.0.0.1 The – is to separate the options of kubernetes to the parameters of the container. . what has been created? . kubectl get all . It doesn’t show everything just the usual suspects. . Pod . A Pod is not a process; it’s an environment for containers | it cannot be “restarted” | it cannot “crash” . | The containers in a Pod can crash | They may or may not get restarted (depending on Pod’s restart policy) | If all containers exit successfully, the Pod ends in “Succeeded” phase | If some containers fail and don’t get restarted, the Pod ends in “Failed” phase | . Replica set . Set of identical pods. | Defined by a pod template + number of desired replicas. | When scale up / down: | we update the manifest of the replica set. | as a consequence, the replica set controller creates / deletes pods. | . Deployment . Deployments roll out different Pods (different image, command, environment vars…) . When we update a deployment with a new Pod definition: | a new replica set is created with the new pod definition. | that new Replica is scaled up. | the old replica set is scaled down. | This is a rolling update, minimizing application downtime. | When we scale up/down a Deployment, it scales up / down its replica set. | . Scaling the deployment: . kubectl scale deployment pingpong –replicas 3 . to check we have multiple pods: kubectl get pods . Scaling a replica set . What if we scale the replica set? . The deployment will notice and will scale back to the initial level. | The replica set makes sure to have the right number of pods. | The deployment makes sure the Replica set has the right size. | . Checking deployment logs . kubectl logs deploy/pingpong –tail 2 . Batch Jobs . Pods don’t get restarted if something goes wrong. Jobs are great for long background jobs. CronJobs are for scheduled jobs. . Creating a job . kubectl create job flipcoin –image=alpine – sh -c ‘exit $(($RANDOM%2))’ . check the status with name selector: kubectl get pods –selector=job-name=flipcoin . We can specify a number of “completions” (default=1) . | We can specify the “parallelism” (default=1) . | . Scheduling periodic background work CronJob . It requires a schedule . minute [0,59] | hour [0,23] | day of the month [1,31] | month of the year[1,12] | day of the week [0,6] 0= Sunday. for example: */3 * * * * means every three minutes. | . Cronjob creation . kubectl create cronjob every3mins –schedule=”*/3 * * * *” –image=alpine – sleep 10 . To check the job: kubectl get cronjobs . The job will create a pod, the job will make sure the pod completes (re-creating another one if it fails). . Setting a time limit: This is done with the field spec.activeDeadlineSeconds. When the job is older than the limit, all its pods are terminated. Note that there can be also a deadline for the pods They can be set independently with different effect: . the deadline of the job will stop the entire job. | the deadline of the pod will stop only a pod. | . Labels and annotations . Set a label on the clock Deployment: kubectl label deployment clock color=blue . kubectl describe deployment clock . List all the labels that we have on pods: . kubectl get pods –show-labels . List the value of label app on these pods: . kubectl get pods -L app . If a selector has multiple labels, it means “match at least these labels” . Example: –selector=app=frontend,release=prod . We can use negative selectors –selector=app!=clock . Can be used with most kubectl commands. . Other ways to get the labels: kubectl get –show-labels po,rs,deploy,svc,no . Differences between labels and annotations . The key for both labels and annotations: . must start and end with a letter or digit can also have . - _ (but not in first or last position) can be up to 63 characters, or 253 + / + 63 . Label values are up to 63 characters Annotations values can have arbitrary characters (yes, even binary) . Maximum length isn’t defined . Kube Logs . kubectl logs only shows the logs of one of the pods. View the last line of log from all pods with the app=pingpong label: . kubectl logs -l app=pingpong –tail 1 The logs can only be streamed for 5 pods. There are external tools to address these shortcomings. Stern https://github.com/wercker/stern . Stern . https://container.training/kube-selfpaced.yml.html#212 .",
            "url": "https://jpons.es/2022/11/05/kubernetes-cheat-sheet",
            "relUrl": "/2022/11/05/kubernetes-cheat-sheet",
            "date": " • Nov 5, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Hugo template for Adobe XD",
            "content": "Creation of a new theme . https://retrolog.io/blog/creating-a-hugo-theme-from-scratch/ . https://gohugo.io/commands/hugo_new_theme/#hugo-new-theme .",
            "url": "https://jpons.es/2022/07/20/Hugo-templates",
            "relUrl": "/2022/07/20/Hugo-templates",
            "date": " • Jul 20, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Firebase emulators cheat sheet",
            "content": "Start emulator localy with saved data . firebase emulators:start --import . data . Save data from emulator to file . firebase emulators:export ./data/ . Sample output . i Found running emulator hub for project orfeondegranada-c6f3e at http://localhost:4400 ? The directory C: Users polit sources orfeonapp data already contains export data. Exporting again to the same director y will overwrite all data. Do you want to continue? Yes i Exporting data to: C: Users polit sources orfeonapp data + Export complete . publish the functions to firebase . firebase deploy --only functions . References: https://firebase.google.com/docs/functions/manage-functions#modify .",
            "url": "https://jpons.es/2022/07/18/firebase-emulators-cheat-sheet",
            "relUrl": "/2022/07/18/firebase-emulators-cheat-sheet",
            "date": " • Jul 18, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Convert Heic files to JPG in windows",
            "content": "Convert Heic files to JPG in windows . Heic format is a proprietary format for images. We can convert it in windows using Gimp software and some batch scripting. Once you installed Gimp, you can use the following batch script on windows to convert all images into jpg. . @echo off REM Find Gimp in the registry for /f &quot;tokens=2*&quot; %%a in (&#39;reg query &quot;HKCR GIMP2.svg shell open command&quot; /ve 2^&gt;^&amp;1^|find &quot;REG_&quot;&#39;) do @set gimp=%%b REM Calculate console exe set gimp=%gimp:gimp-=gimp-console-% REM Isolate exe for %%i in (%gimp%) do ( @set gimp=%%i goto :found ) :found echo Found Gimp console: %gimp% REM Process files (change to &quot;for /r %%i&quot; for recursion) for %%i in (*.heic) do ( echo - Converting [ %%i --^&gt; %%~ni.jpg] %gimp% -i -b &quot;(let* ((image (car (file-heif-load RUN-NONINTERACTIVE &quot;%%i &quot; &quot; &quot; ))) (drawable (car (gimp-image-get-active-layer image)))) (plug-in-autocrop RUN-NONINTERACTIVE image drawable) (gimp-file-save RUN-NONINTERACTIVE image drawable &quot;%%~ni.jpg &quot; &quot;%%~ni.jpg &quot;) (gimp-image-delete image))&quot; -b &quot;(gimp-quit 0)&quot; ) . I just modified the function called to load the heic files. Save this as a .bat file and it will transform all your heic files into jpg. . More information in the following sites: . SuperUser Gimp manual . .",
            "url": "https://jpons.es/2022/07/15/Convert-heic-to-jpg-in-windows",
            "relUrl": "/2022/07/15/Convert-heic-to-jpg-in-windows",
            "date": " • Jul 15, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Installing Algolia in Firebase",
            "content": "Installing Algolia as a search engine for firebase . Firebase is a very popular service from Google. They provice authentication services, cloud databases, analytics for your apps, and there is a really cool integration with Dart/Flutter. . We are using a document database called Firestore. . It’s a cool document database, with some similar concepts to MongoDB, in the sense it groups the elements o the database in documents and collections. What’s different is the grouping of these elements. . First, you define a collection which contains documents. Each document may, at the same time contain attributes and collections. . For instance, my user collection contains a set of documents representing each a user. To register the attendance, each user contains a collection with its own attendances (instead of having an attendance collection and somehow linking the collection to the user). . . In the image, you see, that the user collection, contains documents with the info of each user. The so-called attributes like the user name, email, and so on. But at the user level, that’s at the level of the attributes, you can define collections, in this case a collection for the attendance with a set of attributes. . The problem with this way of organizing the information, compared to the typical canonical, relational table shape, is the way to query the information. . How to query? The search functionality provided with Firestore is quite limited.. . The set of limitations as described in the previous link are the following: . Query limitations . The following list summarizes Cloud Firestore query limitations: . - Cloud Firestore provides limited support for logical OR queries. The in, and array-contains-any operators support a logical OR of up to 10 equality (==) or array-contains conditions on a single field. For other cases, create a separate query for each OR condition and merge the query results in your app. - In a compound query, range (&lt;, &lt;=, &gt;, &gt;=) and not equals (!=, not-in) comparisons must all filter on the same field. - You can use at most one array-contains clause per query. You can&#39;t combine array-contains with array-contains-any. - You can use at most one in, not-in, or array-contains-any clause per query. You can&#39;t combine in , not-in, and array-contains-any in the same query. - You can&#39;t order your query by a field included in an equality (==) or in clause. - The sum of filters, sort orders, and parent document path (1 for a subcollection, 0 for a root collection) in a query cannot exceed 100. . Algolia to the rescue . One of the limitations is full text search for a given attribute. That is, we can’t search for all users starting with Jos. . We can configure Algolia community edition with our Firebase project to run queries like this. The process is as follows: . First, register in Algolia and create a free account. | Create an Algolia application and an index name. | Next, add the Algolia extension to your project. | After the extension is installed, you will find a cloud function that is triggered every document update. But, if your Firestore database has already some data, you have to create another cloud function (that will be triggered maybe just once) to send all your initial data to Algolia index. Some tips for that cloud functions are here | . Create an Algolia application and an index name. . . Obtain the API key . . Add Algolia extension to your firebase project . . . Export all records: https://discourse.algolia.com/t/export-all-records-from-firestore-to-indices-with-google-cloud-function/10358/3 . Implementation in flutter https://www.algolia.com/doc/guides/building-search-ui/getting-started/how-to/flutter/ios/ . Some references: . https://www.algolia.com/doc/api-reference/api-methods/save-objects/#examples . https://www.algolia.com/doc/api-client/getting-started/install/javascript/?client=javascript . Code to generate the index for the first time: . exports.index_all_sheets = functions.runWith({ allowInvalidAppCheckToken: false, // Opt-out: Requests with invalid App // Check tokens continue to your code. }).https.onCall((data, context) =&gt; { const algolia = algoliasearch(&quot;projectID&quot;, &quot;secret&quot;); const index = algolia.initIndex(&quot;sheetIndex&quot;); functions.logger.info(&#39;indexing all sheets&#39;); let records = []; admin.firestore().collection(&quot;partitura&quot;) .get().then((docs)=&gt; { docs.forEach((doc)=&gt;{ const obj = doc.data(); obj.objectID = doc.id; obj.path = &#39;partitura/&#39;+doc.id; obj.compositor = doc.compositor; obj.compositor = doc.obra; records.push(obj); functions.logger.info(&#39;indexing doc&#39;); }); functions.logger.info(&#39;fetch completed&#39;); index.saveObjects(records).then(({objectIDs})=&gt;{ functions.logger.info(&quot;indexing completed&quot;, objectIDs); }); }); return true; }); . Installing Algolia in Flutter project . The first step is to add the Algolia library in your flutter project. You can do so installing the package. . flutter pub add algolia . There are other options, but this one is the fastest for me. Next, I’m going to create an algolia_options.dart file to store the api key (in the same fashion as the Firebase Options). . import &#39;package:algolia/algolia.dart&#39;; class AlgoliaOptions { /// The API key that is used to identify an instance of algolia final String apiKey; /// The application Id in Algolia final String applicationId; const AlgoliaOptions({required this.apiKey, required this.applicationId}); static const AlgoliaOptions algoliaOptions = AlgoliaOptions( apiKey: &quot;xxxxxxxx2&quot;, applicationId: &quot;AAAAAAAAAB&quot;); } . the values of the api key and appId can be found in your Algolia settings page. . Now, I’m going to write the code to search using the algolia index created in the previous steps. First, in the state class I create a private attribute called _algoliaClient . class _PartituraScreenState extends State&lt;PartituraScreen&gt; { final Algolia _algoliaClient = Algolia.init( applicationId: AlgoliaOptions.algoliaOptions.applicationId, apiKey: AlgoliaOptions.algoliaOptions.apiKey); TextEditingController _textFieldController = TextEditingController(); . I’m adding a text controller for the filter. I’m also creating a proxy class for the Sheet class: . class SheetProxy { final String id; final String obra; final String compositor; SheetProxy({required this.id, required this.obra, required this.compositor}); static SheetProxy fromJson(Map&lt;String, dynamic&gt; json) { final String id = json[&#39;path&#39;].toString().split(&quot;/&quot;)[1]; return SheetProxy( id: id, obra: json[&#39;obra&#39;], compositor: json[&#39;compositor&#39;]); } } . The function to search the elements is the following, that maps the content to the SheetProxy instance. . Future&lt;void&gt; _getSearchResult(String query) async { AlgoliaQuery algoliaQuery = _algoliaClient.instance.index(&quot;sheetIndex&quot;).query(query); AlgoliaQuerySnapshot snapshot = await algoliaQuery.getObjects(); final rawHits = snapshot.toMap()[&#39;hits&#39;] as List; final hits = List&lt;SheetProxy&gt;.from(rawHits.map((hit) =&gt; SheetProxy.fromJson(hit))); setState(() { _sheets = hits; }); print(rawHits); } . Finally, on the init state, we add a listener to the text field controller: . @override void initState() { super.initState(); _textFieldController.addListener(() { if (_query != _textFieldController.text) { setState(() { _query = _textFieldController.text; }); _getSearchResult(_query); } }); _getSearchResult(&#39;&#39;); } . The application with the filtering looks like this: . .",
            "url": "https://jpons.es/2022/02/12/Fulltext-search-Firestore-with-Algolia",
            "relUrl": "/2022/02/12/Fulltext-search-Firestore-with-Algolia",
            "date": " • Feb 12, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "AWS commands cheat sheet",
            "content": "AWS commands cheat sheet . S3 . Max filesize upload in GUI 160GB. . AWS CLI . Configure . aws configure . Create a bucket . aws s3api create-bucket --bucket bucketName --region frankfurt --create-bucket-configuration LocationConstraint=frankfurt . list buckets . aws s3api list-buckets --query &quot;Buckets[].Name&quot; . upload files . aws s3 cp d: localfile s3://bucketname --recursive --exclude &quot;*&quot; --include &quot;*.txt&quot; . list files . aws s3 ls s3://bucket . Change storage class . aws s3 cp s3://bucketName s3://bucketName --storage-class GLACIER . Set encryption . aws s3 cp s3://bucketName/file.txt s3://bucketName/file.txt --sse AES256 . To apply to the entire bucket recursively . aws s3 cp s3://bucketName/ s3://bucketName/ --recursive --sse AES256 . Network ACL management . aws ec2 describe-vpcs --output table aws ec2 create-network-acl --vpc-id vpc-12312321 . To give it a name . aws ec2 create-tags --resources acl-asdasd --tags Key=Name,Value=NetworkACL1 . to create a network rule: . aws ec2 create-network-acl-entry --network-acl-id acl-afdadf --ingress --rule-number 100 --protocol tcp --port-range From=22,To=22 --cidr-block 0.0.0.0/0 --rule-action allow . Security Group in the CLI . Get the vpc ID . aws ec2 create-security-group --group-name SecurityGroup1 --description &quot;Security Group&quot; --vpc-id vpc-asdasd . we will get the group id . aws ec2 describe-security-groups --output table . Tag the security group . aws ec2 create-tags --resources sg-asdfasdfasdf --tags Key=Name,Value=SecGroup1 aws ec2 authorize-security-group-ingress --group-id sg-asdfasdf --protocol tcp --port 3380 --cidr 100.11.11.0/24 . VPC creation . aws ec2 create-vpc --cidr-block 12.0.0.0/16 . Get the vpc id: . aws ec2 describe-vpcs . Add tags . aws ec2 create-tags --resources vpc-idididid --tags Key=Name,Value=VPC2 aws ec2 create-subnet --vpc-id vpc-001010101 --cidr-block 12.0.1.0/24 . Add tags to the subnet: . aws ec2 create-tags --resources subnet-idididid --tags Key=Name,Value=Subnet2 . EC2 instances . We need to get the id of the AMI first . aws ec2 run-instances --image-id ami-asdasda --count 1 --instance-type t2.micro --key-name Keypair1 --security-groups-ids sg-asdfas --subnet-id subnet-asdasd . to update the name, we can use the tags. . RDS on the CLI . aws rds help aws rds describe-db-instance --output table | more aws rds start-db-instance --db-instance-identifier database-1 . To see the current status we can run the previous command . IAM create user . aws create-user --user-name JGold . IAM add user to group . aws iam add-user-to-group --user-name JGold group-name Group1 . IAM get user information . aws iam get-user //For your own user aws iam get-user --user-name JGold . IAM list groups for user . aws iam list-groups-for-user --user-name JGold . IAM crete group . aws iam create-group --group-name Group4 . get group . aws iam get-group --group-name Group4 . IAM add user to group . aws iam add-user-to-group --user-name JGold --group-name Group4 . Explore costs using the GUI . We have the following config file: . { &quot;Dimensions&quot;: { &quot;Key&quot; : &quot;SERVICE&quot;, &quot;Values&quot;: [ &quot;Amazon Elastic Compute Cloud - Compute&quot;] } } . aws ce get-cost-and-usage --time-period Start=2019-09-01,End=2019-12-01 --granularity MONTHLY --metrics &quot;BlendedCost&quot; &quot;UnblendedCost&quot; &quot;UsageQuantity&quot; --group-by Type=DIMENSION,Key=SERVICE Type=TAG,Key=Environment --filter file://aws_cost_filter.json --output table . AWS PowerShell CLI . Initialize the connection . initialize-awsdefaults -region us-east-1 . Search for a command . Get-Command *s3b* . Create a bucket . New-S3Bucket -BucketName pp -Region us-west-2 . list buckets . Get-S3Bucket . upload a file . Write-S3Object -BucketName name -File filename -Key localfile -CannedACLName Private . list files . Get-S3Object -BucketName name -Key parentFolder Get-S3Object -BucketName name -Key parentFolder | select Key . change storage class . Copy-S3Object -BucketName bucket -Key file.txt -DestinationKey file.txt -StorageClass GLACIER . set encryption . This rule add encryption for new items in the bucket but do not change . Set-S3BucketEncryption -BucketName bucketName -ServerSideEncryptionConfiguration_ServerSideEnctryptionRule @{ServerSideEncryptionByDefault=@{ServerSideEncryptionAlgorithm=&quot;AES256&quot;}} . ` . Network ACL in Powershell . Get-EC2VPC . to get the VPC ID . New-EC2NetworkAcl -VpcId vpc-asdasd . to get the network ACL ID . New-EC2Tag -ResourceId acl-asdasd -Tag @{Key=&quot;Name&quot;;Value=&quot;NetworkACL4&quot;} . to add traffic: . New-EC2NetworkAclEntry -NetworkAclId acl-sfsdf -Egress $false -RuleNumner 100 -Protocol 6 -PortRange_From 443 -PortRange_To 443 CidrBlock 199.111.111.111/24 -RuleAction allow . Note: . protocol 6 is for TCP | 70 is for UDP | 1 for ICMP | . Security group . Get the vpc ID | Create the security group | . New-EC2SecurityGroup -GroupName secgroupname -Description &quot;DEscription&quot; -VpcId vpc-asdfasdf . It returns a sec group id . New-EC2Tag -ResourceId sg-asfdasdf -Tag @{Key=&quot;Name&quot;;Value=&quot;SecurityGroup3&quot;} . To create a rule: . $rule1 = @{IPProtocol=&quot;tcp&quot;;FromPort=&quot;22&quot;;ToPort=&quot;22&quot;;IpRanges=&quot;199.11.11.0/24&quot;} . To apply . Grant-EC2SecurityGroupIngress -GroupId sg-fasfd -IpPermission $rule1 . VPC creation . New-EC2VPC -CidrBlock 13.0.0.0/16 . Give a name with the tags: . New-EC2Tag -ResourceId vpc-asdasd -Tag @{Key=&quot;Name&quot;;Value=&quot;VPC3&quot;} . Create a subnet: . New-EC2Subnet -VpcId vpc-asda -CidrBlock 13.0.1.0/24 . Give a name to the subnet . New-EC2Tag -ResourceId subnet-asdasd -Tag @{Key=&quot;Name&quot;;Value=&quot;subnet21&quot;} . EC2 creation . New-EC2Instance -ImageId ami-asdfasdf -MinCount 1 -MaxCount 1 -KeyName KeyPair1 -SecurityGroupId sg-asdf -InstanceType m1.small -SubnetId subnet-asfas . Add a tag to add a name. . Get Status RDS Database . Get-RDSDBInstance . To do a selection . Get-RFSDBInstance | select engine,dbinstancestatus Get-RFSDBInstance | select DBInstanceIdentifier,Engine,EngineVersion | where-object {$_.Engine -like &quot;*mysql*&quot;} . Start RDS InstanceType . Start-RDSDBInstance . IAM create user . New-IAMUser -UserName MBishop . Get user . Get-IAMUser Get-IAMUer -Username MBishop . IAM add user to group . Add-IAMUserToGroup -UserName MBishop -GroupName Group1 . IAM get group . Get-AIMGroup -GroupName Group1 . IAM crete group . New-IAMGroup -GroupName Group2 . IAM get group policies . Get-IAMGroupPolicies -GroupName Group1 . The attched group policies can be obtain here: . Get-IAMattachedgrouppolicies -groupname Group1 . Explore costs using PowerShell . first, define a time interval: | . $interval = New-Object Amazon.CostExplorer.Model.DateInterval $interval.Start = Get-Date (Get-Date).AddDays(-30) -Format &#39;yyyy-MM-dd&#39; $interval.End = Get-Date -Format &#39;yyyy-MM-dd&#39; . to get the cost: . $costusage = get-cecostusage -granularity monthly -timeperiod $interval -metric BlendedCost . To check the values: . $costusage.resultsbytime.total.values .",
            "url": "https://jpons.es/2021/12/23/CLI-cheat-sheet",
            "relUrl": "/2021/12/23/CLI-cheat-sheet",
            "date": " • Dec 23, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Kore.AI export knowledge base into batch testing",
            "content": "How to export the knowledge base and transform into batch testing . We need to transform between 2 JSON files. The knowledge base can be extracted from the tool, and the Batch testing only needs some items from there. . We will be using the Google Fire library to wrap our code. . import json import fire def create_test(knowledge_base,outfile): with open(knowledge_base,&#39;r&#39;,encoding=&#39;utf-8&#39;) as f: data = json.load(f) tests = {&#39;testCases&#39;:[]} for term in data[&#39;faqs&#39;]: key = term[&#39;question&#39;] tests[&#39;testCases&#39;].append({&#39;input&#39;:key,&#39;intent&#39;:key}) for alt in term[&#39;alternateQuestions&#39;]: if not alt[&#39;question&#39;].startswith(&#39;||&#39;): tests[&#39;testCases&#39;].append({&#39;input&#39;:alt[&#39;question&#39;],&#39;intent&#39;:key}) with open(outfile,&#39;w&#39;,encoding=&#39;utf-8&#39;) as out: json.dump(tests,out) if __name__ == &#39;__main__&#39;: fire.Fire(create_test) . Resources . programmiz . stackoverflow . Python Fire . String comparison .",
            "url": "https://jpons.es/2021/03/29/export-knowledge-base-koreai",
            "relUrl": "/2021/03/29/export-knowledge-base-koreai",
            "date": " • Mar 29, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "MongoDB commands",
            "content": "Connect to mongo using SSL certificates . mongo --host hostname --port 27027 -ssl --sslPEMKeyFile /path/to/file --sslCAFile /path/to/file . Login as admin user . use admin db.auth(&#39;user&#39;,&#39;passwd&#39;) . show databases . show databases . show collections . show collections . explore collections . db.collectionname.find() .",
            "url": "https://jpons.es/2021/03/26/mongodb-commands",
            "relUrl": "/2021/03/26/mongodb-commands",
            "date": " • Mar 26, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "How to get webservice times from curl",
            "content": "How to get times from curl . We want to do a POST query to Webex APIs, and we want to measure the response time. To do that, we need to write a config file: . { n &quot;time_redirect&quot;: %{time_redirect}, n &quot;time_namelookup&quot;: %{time_namelookup}, n &quot;time_connect&quot;: %{time_connect}, n &quot;time_appconnect&quot;: %{time_appconnect}, n &quot;time_pretransfer&quot;: %{time_pretransfer}, n &quot;time_starttransfer&quot;: %{time_starttransfer}, n &quot;time_total&quot;: %{time_total}, n &quot;size_request&quot;: %{size_request}, n &quot;size_upload&quot;: %{size_upload}, n &quot;size_download&quot;: %{size_download}, n &quot;size_header&quot;: %{size_header} n } n . Save it as curlFormat.txt, then you can run your curl query as . curl -H &quot;Content-Type: application/json&quot; -w &quot;@curlFormat.txt&quot; -H &quot;Authorization: Bearer xxxxxxxxxx&quot; -X POST -d &#39;{&quot;roomId&quot;:&quot;xxxxxxxxxxxx&quot;,&quot;text&quot;:&quot;test&quot;}&#39; &#39;https://webexapis.com/v1/messages&#39; . Resources . StackOverflow https://discuss.devopscube.com/t/how-to-find-response-time-using-curl-request/436 .",
            "url": "https://jpons.es/2021/03/15/curl",
            "relUrl": "/2021/03/15/curl",
            "date": " • Mar 15, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Vim shortcuts",
            "content": "Cut/copy and paste using visual selection . Visual selection is a common feature in applications, but Vim’s visual selection has several benefits. . To cut-and-paste or copy-and-paste: . Position the cursor at the beginning of the text you want to cut/copy. Press v to begin character-based visual selection, or V to select whole lines, or Ctrl-v or Ctrl-q to select a block. Move the cursor to the end of the text to be cut/copied. While selecting text, you can perform searches and other advanced movement. Press d (delete) to cut, or y (yank) to copy. Move the cursor to the desired paste location. Press p to paste after the cursor, or P to paste before. . Visual selection (steps 1-3) can be performed using a mouse. . If you want to change the selected text, press c instead of d or y in step 4. In a visual selection, pressing c performs a change by deleting the selected text and entering insert mode so you can type the new text. . Comment and uncomment . Put your cursor on the first # character, press CtrlV (or CtrlQ for gVim), and go down until the last commented line and press x, that will delete all the # characters vertically. . For commenting a block of text is almost the same: . First, go to the first line you want to comment, press CtrlV. This will put the editor in the VISUAL BLOCK mode. Then using the arrow key and select until the last line Now press ShiftI, which will put the editor in INSERT mode and then press #. This will add a hash to the first line. Then press Esc (give it a second), and it will insert a # character on all other selected lines. . For the stripped-down version of vim shipped with debian/ubuntu by default, type : s/^/# in the third step instead (any remaining highlighting of the first character of each line can be removed with :nohl). . Here are two small screen recordings for visual reference. . Replace all . The :substitute command searches for a text pattern, and replaces it with a text string. There are many options, but these are what you probably want: . :s/foo/bar/g . Find each occurrence of &#39;foo&#39; (in the current line only), and replace it with &#39;bar&#39;. . :%s/foo/bar/g . Find each occurrence of &#39;foo&#39; (in all lines), and replace it with &#39;bar&#39;. . :%s/foo/bar/gc . Change each &#39;foo&#39; to &#39;bar&#39;, but ask for confirmation first. . :%s/ &lt;foo &gt;/bar/gc . Change only whole words exactly matching &#39;foo&#39; to &#39;bar&#39;; ask for confirmation. . :%s/foo/bar/gci . Change each &#39;foo&#39; (case insensitive due to the i flag) to &#39;bar&#39;; ask for confirmation. :%s/foo c/bar/gc is the same because c makes the search case insensitive. This may be wanted after using :set noignorecase to make searches case sensitive (the default). . References . Visual selection Comment and uncomment .",
            "url": "https://jpons.es/2021/03/08/vim",
            "relUrl": "/2021/03/08/vim",
            "date": " • Mar 8, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "WebEx Teams",
            "content": "Webex Teams . Today, we will implement some webservices to emulate a bot in webex. First, we need to design two webservices in nodeJS, then we will synchronize them. For that, we will use nodeJS. . The main site for Webex Teams for developers is here. There you can define integrations, bots and check the documentation for the API. . required libraries . var express = require(&#39;express&#39;); var router = express.Router(); var request = require(&#39;request-promise&#39;); . We are using express and request-promise. . messages and API call . We test two webservices: the people API and the message post. We compose the mesage in two steps: . First, we compose the body (bodyJSON) with the required information for the API call (message content, the toPersonId value, etc). | Then, we create the headers and wrap both headers and body into one single object. | . bodyJson = {&quot;markdown&quot;: message.replace(/ n r| n| r/g,&#39;&lt;br&gt;&#39;).replace(/-/g,&quot;&amp;#45;&quot;)}; to = &#39;123123abmasdf&#39;; // this is your user id bodyJson.toPersonId = to; . Then, the request can be found here: . var reqOptions = { url: config.spark.baseUrl + &quot;messages&quot;, method: &quot;POST&quot;, headers: { &#39;Authorization&#39;: &quot;Bearer &quot; + sparkChannel.authToken, &#39;Content-Type&#39;: &quot;application/json; charset=utf-8&quot; }, body: JSON.stringify(bodyJson) }; . Webservice call sync . To synchronize two webservice calls, the first one to the people API and the second one to the messages api, we use javascript promises: . We have promise p1 for the check to the people api and p2 to send the message. The logic here is that p1 check for user permissions while p2 just send the message if the user is authorized. . var p1 = request(reqOptions1).then(function(result){ result = JSON.parse(result); var authorized = false; if(result.items[0].orgId === orgId){ authorized = true; userMap[to]={}; } return authorized; }); var p2 = p1.then(function(result){ if(result){ return request(reqOptions); }else{return;} }); return Promise.join(p1,p2,function(results1,results2){ return; }); .",
            "url": "https://jpons.es/2021/03/01/webex-teams",
            "relUrl": "/2021/03/01/webex-teams",
            "date": " • Mar 1, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Export excel to csv in python",
            "content": "How to export Excel file to csv in python . We use the pandas library and the following three lines: . import pandas as pd df = pd.read_excel(&quot;Libro2.xlsx&quot;,sheet=&quot;questionlist&quot;) df.to_csv(&#39;l2.csv&#39;,index=False,header=False) . Sometimes we need to do some formatting before we export the excel to csv file. . Convert a column to string type . df.Answers = df.Answers.astype(str) . Check the original in stackoverflow. . Remove new lines . df.Answers = df.Answers.str.replace(r&quot;/ n r| n| r/g&quot;,&#39;&#39;) . Check the syntax of replace here. . In case you want to modify a column, you can do the following to replace some characters: . df.Answers = df.Answers.str.replace(&quot; &quot;&quot;,&quot;&quot;) . Remove all spaces: . df.Question.apply(lambda x: &quot; &quot;.join(x.split())) . Remove trailing spaces . df.Answers = df.Answers.str.strip() . Example program in python . import pandas as pd import fire def to_csv(fileName,sheet,outfile): df = pd.read_excel(fileName,sheet=sheet) for c in df.columns: df[c]= df[c].astype(str) df[c] = df[c].str.replace(r&quot;/ n r| n| r/g&quot;,&#39;&#39;) df[c] = df[c].str.strip() df.to_csv(outfile,index=False,header=False) if __name__ == &#39;__main__&#39;: fire.Fire(to_csv) . Documentation from pandas .",
            "url": "https://jpons.es/2021/02/20/export-excel-to-csv-in-python",
            "relUrl": "/2021/02/20/export-excel-to-csv-in-python",
            "date": " • Feb 20, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "Set proxy in conda",
            "content": "Set proxy in conda . Some companies run within a proxy, to configure conda so you can install packages, take the following file: C: Users username.condarc . Add the line proxy_servers with the IP and port: . ssl_verify: true channels: - conda-forge - defaults proxy_servers: http: http://10.49.1.1:8080/ https: http://10.49.1.1:8080/ . Set proxy in pip . (base) C: Users x&gt;SET http_proxy=http://10.49.1.1:8080/ (base) C: Users x&gt;echo %http_proxy% http://10.49.1.1:8080/ (base) C: Users x&gt;SET https_proxy=http://10.49.1.1:8080/ (base) C: Users x&gt;pip install fire Collecting fire .",
            "url": "https://jpons.es/2021/02/15/set-proxy-in-conda",
            "relUrl": "/2021/02/15/set-proxy-in-conda",
            "date": " • Feb 15, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "Troubleshooting webservice time",
            "content": "Troubleshooting webservice at OS level . We are trying to troubleshoot a webservice call that’s taking a lot of time. The initial idea it that the processing time in our platform is the issue. Let’s use curl at OS level with the configuration as explained here to troubleshoot. . This is the answer we get from the curl command: . { &quot;time_redirect&quot;: 0.000, &quot;time_namelookup&quot;: 5.515, &quot;time_connect&quot;: 5.630, &quot;time_appconnect&quot;: 6.008, &quot;time_pretransfer&quot;: 6.008, &quot;time_starttransfer&quot;: 6.385, &quot;time_total&quot;: 6.385, &quot;size_request&quot;: 375, &quot;size_upload&quot;: 103, &quot;size_download&quot;: 375, &quot;size_header&quot;: 343 } . Here we see 5+ seconds in the name resolution. A first workaround is to get the IP address of the target and bypass the name server by using the hostname file: . nslookup webexapis.com . we will get several answers: . Server: 100.125.4.25 Address: 100.125.4.25#53 Non-authoritative answer: Name: webexapis.com Address: 3.130.32.130 Name: webexapis.com Address: 3.139.30.165 Name: webexapis.com Address: 3.140.133.193 . we edit the /etc/hosts file and add that IP and host: . 3.140.133.193 webexapis.com . if now we issue again the command, then let’s see if the issue is solved: . { &quot;time_redirect&quot;: 0.000, &quot;time_namelookup&quot;: 0.005, &quot;time_connect&quot;: 0.118, &quot;time_appconnect&quot;: 0.507, &quot;time_pretransfer&quot;: 0.507, &quot;time_starttransfer&quot;: 0.877, &quot;time_total&quot;: 0.877, &quot;size_request&quot;: 375, &quot;size_upload&quot;: 103, &quot;size_download&quot;: 375, &quot;size_header&quot;: 343 } . It seems the issue is related to the name server. Next step would be to troubleshoot the name server and see why it’s taking 5 seconds to resolve that url. .",
            "url": "https://jpons.es/2021/02/05/troubleshooting-webservice-time",
            "relUrl": "/2021/02/05/troubleshooting-webservice-time",
            "date": " • Feb 5, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "How do I know the number of bits and encruption of my public key?",
            "content": "How do I know the number of bits and encruption of my public key? . According to superuser . ssh-keygen -l -f ~/.ssh/id_rsa.pub .",
            "url": "https://jpons.es/2021/01/30/ssh-check-bits",
            "relUrl": "/2021/01/30/ssh-check-bits",
            "date": " • Jan 30, 2021"
        }
        
    
  
    
        ,"post17": {
            "title": "Create CLI for python",
            "content": "Create CLI for python using Fire . Fire is a really simple library to build CLI. Install it ( you can use conda as well): . pip install fire . And then use it in your app, for example here: . import pandas as pd import fire def to_csv(fileName,sheet,outfile): df = pd.read_excel(fileName,sheet=sheet) for c in df.columns: df[c]= df[c].astype(str) df[c] = df[c].str.replace(r&quot;/ n r| n| r/g&quot;,&#39;&#39;) df[c] = df[c].str.strip() df.to_csv(outfile,index=False,header=False) if __name__ == &#39;__main__&#39;: fire.Fire(to_csv) .",
            "url": "https://jpons.es/2020/12/15/create-cli-for-python",
            "relUrl": "/2020/12/15/create-cli-for-python",
            "date": " • Dec 15, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "Some links to configure log rotation",
            "content": "Resources . configure log rotate Issues with Log rotate Manage log files .",
            "url": "https://jpons.es/2020/08/15/log-rotation",
            "relUrl": "/2020/08/15/log-rotation",
            "date": " • Aug 15, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://jpons.es/2020/02/20/test",
            "relUrl": "/2020/02/20/test",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post20": {
            "title": "Installing caffe in Ubuntu 16.04",
            "content": "Caffe is a library used in deep learning and specific to computer vision. They provide an installation guide, which is quite nice. But I experienced problems during the install. You can download the source code here . First follow the installation instructions. . I had installed cuda 8 library directly from Nvidia and not from ubuntu default repos. . Then install the following packages: . sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler sudo apt-get install --no-install-recommends libboost-all-dev . And the BLAS library of your choice, I selected by default ATLAS: . sudo apt-get install libatlas-base-dev . As I’m going to work with python, I prepared a virtual environment. This is also because I work with tensorflow and I don’t want to mix both. Assuming you have installed virtualenv: . virtualenv caffe_gpu source ~/caffe_gpu/bin/activate . Assuming your code is in ~/caffe: . cd python for req in $(cat requirements.txt); do pip install $req; done . Then, I had to adapt the make file: . cp Makefile.config.example Makefile.config . adjust: . USE_CUDNN := 1 INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial/ LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-linux-gnu/hdf5/serial/ . finally do a symbolic link: . sudo ln -s /usr/lib/x86_64-linux-gnu/libhdf5_serial_hl.so /usr/lib/x86_64-linux-gnu/libhdf5_hl.so sudo ln -s /usr/lib/x86_64-linux-gnu/libhdf5_serial.so /usr/lib/x86_64-linux-gnu/libhdf5.so . Now we can compile as in the instructions: . make all make test make runtest . Thanks for reading! .",
            "url": "https://jpons.es/2017/01/03/installing-caffe-in-ubuntu-16-04",
            "relUrl": "/2017/01/03/installing-caffe-in-ubuntu-16-04",
            "date": " • Jan 3, 2017"
        }
        
    
  
    
        ,"post21": {
            "title": "Getting started with docker in Ubuntu 14.04 LTS",
            "content": "Getting started with docker in Ubuntu 14.04 LTS . I’m trying to install a small lab system based on Ubuntu. The Docker project is a new step on the virtualization world, which reduces the amount of resources used by the ‘guest O.S’ since there is no such O.S. . To get an overview on Docker, I recommend to get to the project page and get the resources and the free (an really good) training from there (https://www.docker.com/) . I’m going to follow the instructions on the site to install docker for linux, but I’d like to detail here the problems and the full install I did. . Get to a console and install with the Ubuntu script: . sudo wget -qO- https://get.docker.com/ | sh . Next you can add your own user to run Docker without root or sudo. To do so: If you would like to use Docker as a non-root user, you should now consider adding your user to the “docker” group with something like: . sudo usermod -aG docker jpons Remember that you will have to log out and back in for this to take effect! . Trying to run hello world: . docker run hello-world Post http:///var/run/docker.sock/v1.19/containers/create: dial unix /var/run/docker.sock: no such file or directory. Are you trying to connect to a TLS-enabled daemon without TLS? . Don’t forget to start the Docker server, otherwise even the hello world app won’t start. . jpons@bugambilla:~$ sudo docker run hello-world Unable to find image &#39;hello-world:latest&#39; locally latest: Pulling from hello-world a8219747be10: Pull complete 91c95931e552: Already exists hello-world:latest: The image you are pulling has been verified. Important: image verification is a tech preview feature and should not be relied on to provide security. Digest: sha256:aa03e5d0d5553b4c3473e89c8619cf79df368babd18681cf5daeb82aab55838d Status: Downloaded newer image for hello-world:latest Hello from Docker. This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the &quot;hello-world&quot; image from the Docker Hub. (Assuming it was not already locally available.) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash For more examples and ideas, visit: http://docs.docker.com/userguide/ .",
            "url": "https://jpons.es/2016/09/10/getting-started-ubuntu-14-04-lts",
            "relUrl": "/2016/09/10/getting-started-ubuntu-14-04-lts",
            "date": " • Sep 10, 2016"
        }
        
    
  
    
        ,"post22": {
            "title": "Create an SSL certificate with openSSL",
            "content": "Create an SSL certificate with openSSL . Create a selfsigned SSL certificate . With openssl: . openssl req -x509 -nodes -days 7300 -newkey rsa:2048 -keyout /etc/ssl/certs/vsftpd.pem -out /etc/ssl/certs/vsftpd.pem .",
            "url": "https://jpons.es/2016/09/05/Create-an-SSL-certificate-with-openSSL",
            "relUrl": "/2016/09/05/Create-an-SSL-certificate-with-openSSL",
            "date": " • Sep 5, 2016"
        }
        
    
  
    
        ,"post23": {
            "title": "Testing PAM service authentication",
            "content": "Testing PAM service authentication . #include &lt;stdio.h&gt; #include &lt;security/pam_appl.h&gt; #include &lt;unistd.h&gt; #include &lt;stdlib.h&gt; #include &lt;string.h&gt; struct pam_response *reply; // //function used to get user input int function_conversation(int num_msg, const struct pam_message **msg, struct pam_response **resp, void *appdata_ptr) { *resp = reply; return PAM_SUCCESS; } int authenticate_system(const char *username, const char *password, const char *service_name) { const struct pam_conv local_conversation = { function_conversation, NULL }; pam_handle_t *local_auth_handle = NULL; // this gets set by pam_start int retval; retval = pam_start(service_name , username, &amp;local_conversation, &amp;local_auth_handle); if (retval != PAM_SUCCESS) { printf(&quot;pam_start returned: %d n &quot;, retval); return 0; } reply = (struct pam_response *)malloc(sizeof(struct pam_response)); reply[0].resp = strdup(password); reply[0].resp_retcode = 0; retval = pam_authenticate(local_auth_handle, 0); if (retval != PAM_SUCCESS) { if (retval == PAM_AUTH_ERR) { printf(&quot;Authentication failure. n&quot;); } else { printf(&quot;pam_authenticate returned %d n&quot;, retval); } return 0; } printf(&quot;Authenticated. n&quot;); retval = pam_end(local_auth_handle, retval); if (retval != PAM_SUCCESS) { printf(&quot;pam_end returned n&quot;); return 0; } return 1; } int main(int argc, char** argv) { char* login; char* password; char* service_name; printf(&quot;Authentication module n&quot;); if (argc != 4) { printf(&quot;Invalid count of arguments %d. n&quot;, argc); printf(&quot;./authModule &lt;username&gt; &lt;password&gt; &lt;service_name&gt;&quot;); return 1; } login = argv[1]; password = argv[2]; service_name = argv[3]; if (authenticate_system(login, password,service_name) == 1) { printf(&quot;Authenticate with %s - %s through system n&quot;, login, password); return 0; } printf(&quot;Authentication failed! n&quot;); return 1; } . Now it’s time to compile it: . [root@archpepex ~]# gcc -o authModule authModule.c -lpam [root@archpepex ~]# ./authModule user pass vsftpd .",
            "url": "https://jpons.es/2016/08/30/testing-pam-service-authentication",
            "relUrl": "/2016/08/30/testing-pam-service-authentication",
            "date": " • Aug 30, 2016"
        }
        
    
  
    
        ,"post24": {
            "title": "Read and write your contacts from your SIM mobile card",
            "content": "Read and write your contacts from your SIM mobile card . If you have a smart card reader, you can read the contacts from your SIM card from your mobile phone .",
            "url": "https://jpons.es/2016/08/26/read-write-contacts-sim-mobile-card",
            "relUrl": "/2016/08/26/read-write-contacts-sim-mobile-card",
            "date": " • Aug 26, 2016"
        }
        
    
  
    
        ,"post25": {
            "title": "Upload and download files to FTPs using C#",
            "content": "Upload and download files to FTPs using C# . Check if a file exists How to write to a text file How to download files from FTP How to sleep a thread . Problems related to FTP with SSLIgnore SSL validation for certificates: . SSLIgnore . Configure FTP over SSL . Ignore web certificates . Library for FTPs on C# .",
            "url": "https://jpons.es/2016/07/26/upload-download-files-ftps-c",
            "relUrl": "/2016/07/26/upload-download-files-ftps-c",
            "date": " • Jul 26, 2016"
        }
        
    
  
    
        ,"post26": {
            "title": "SSH authentication with keys",
            "content": "SSH authentication with keys . Handling errors on ssh authentication. . If you have ssh authentication with public / private keys, sometimes you might face the issue, that even when the authorized_keys contains your public key on the target host, the system is asking the password. . Some things you can try: . Copy your public key by using ssh-copy-id command. | Check permissions: the target home folder should be at least 75x, so the group can’t write. | you can debug the output of the ssh authentication command by: | . ssh -v jpons@example OpenSSH_7.2p2, OpenSSL 1.0.2g 1 Mar 2016 debug1: Reading configuration data /home/jpons/.ssh/config debug1: Reading configuration data /etc/ssh/ssh_config debug1: Connecting to example [10.50.136.10] port 22. debug1: Connection established. debug1: identity file /home/jpons/.ssh/id_rsa type 1 debug1: key_load_public: No such file or directory debug1: identity file /home/jpons/.ssh/id_rsa-cert type -1 debug1: key_load_public: No such file or directory debug1: identity file /home/jpons/.ssh/id_dsa type -1 debug1: key_load_public: No such file or directory debug1: identity file /home/jpons/.ssh/id_dsa-cert type -1 debug1: key_load_public: No such file or directory debug1: identity file /home/jpons/.ssh/id_ecdsa type -1 debug1: key_load_public: No such file or directory debug1: identity file /home/jpons/.ssh/id_ecdsa-cert type -1 debug1: key_load_public: No such file or directory debug1: identity file /home/jpons/.ssh/id_ed25519 type -1 debug1: key_load_public: No such file or directory debug1: identity file /home/jpons/.ssh/id_ed25519-cert type -1 debug1: Enabling compatibility mode for protocol 2.0 debug1: Local version string SSH-2.0-OpenSSH_7.2 debug1: Remote protocol version 2.0, remote software version OpenSSH_6.2 debug1: match: OpenSSH_6.2 pat OpenSSH* compat 0x04000000 debug1: Authenticating to example:22 as &#39;orapws&#39; debug1: SSH2_MSG_KEXINIT sent debug1: SSH2_MSG_KEXINIT received debug1: kex: algorithm: ecdh-sha2-nistp256 debug1: kex: host key algorithm: ecdsa-sha2-nistp256 debug1: kex: server-&gt;client cipher: aes128-ctr MAC: umac-64-etm@openssh.com compression: none debug1: kex: client-&gt;server cipher: aes128-ctr MAC: umac-64-etm@openssh.com compression: none debug1: sending SSH2_MSG_KEX_ECDH_INIT debug1: expecting SSH2_MSG_KEX_ECDH_REPLY debug1: Server host key: ecdsa-sha2-nistp256 SHA256:ig7JOjjK6WgLs0FG/OonjCtoU8fyQjFpN45KkCwnIUA debug1: Host &#39;example&#39; is known and matches the ECDSA host key. debug1: Found key in /home/jpons/.ssh/known_hosts:12 debug1: rekey after 4294967296 blocks debug1: SSH2_MSG_NEWKEYS sent debug1: expecting SSH2_MSG_NEWKEYS debug1: rekey after 4294967296 blocks debug1: SSH2_MSG_NEWKEYS received debug1: SSH2_MSG_SERVICE_ACCEPT received .",
            "url": "https://jpons.es/2016/06/08/ssh-authentication-with-keys",
            "relUrl": "/2016/06/08/ssh-authentication-with-keys",
            "date": " • Jun 8, 2016"
        }
        
    
  
    
        ,"post27": {
            "title": "Big Data Lab",
            "content": "Big Data Lab . Setting up your (small) Big Data Laboratory . I want to learn and work with some of the newest ‘Big Data’ tools, like Hadoop, Mahout, Flume, Elastic, etc. But how to do that? Which tecnology should I work with? Is there like a faction of people that only work with Apache products, other just with Amazon. To choose, and experiment with those technologies I’m going to set up a budget lab at home. . I think it’s really important to know what you want to achieve with that lab. First of all, what do you want to do? Data Mining? Machine learning? Classification? It’s a great idea to have a toy project to work with. . In my case I want to do the following: . First, my data set is a collection of terminal outputs with the commands and the output of that commands on different servers. | On an early stage I want to do a Data Analysis. I want to check if there are any identifiable relationships among the commands I send to the terminal. Is there any pattern? May I identify clearly the steps when installing a component (i.e. Apache web server) than when I’m resolving an issue (i.e. Apache went down)? . | The possibilities are countless. Maybe after the data analisys I can train a system to learn on how an installation is performed, or how an issue is solved. After that I could integrate the system and do some stream mining and get suggestions on real time on how to solve an issue. | . But first things first. Taking a look to Data Analysis, there are several popular tools, like R, which provides both user interface and a language and libraries to handle data. A reason to choose R is the quick way to inspect and get some nice plots to explore our data. Also, we can consider Hadoop as engine to store our data sets and also to explore them. From my point of view this option is a little bit more complex at the beggining, but more powerful if you want to explore then some solutions with Mahout, Hive or Flume. . What do I need for my labo? You need hardware and software products. If you plan to install R, then your laptop would do the job. But if you want to work with hadoop, I recommend you to work with Virtual Machines. I always try to work with open source solutions so, to create VM, I’ll use VirtualBox. . Ok, I’m going to installs VMs, which OS should I install? There is a great deal of linux flavours to install your VM. I’ll cover this step in more detail, but as far as I can see the most used OS on cloud environments are: . Ubuntu Server | Red Hat (RHEL) / CentOS I also like to work with Devian and Arch due to the low footprint. | . Some useful links: . Different hadoop installation manuals: http://doctuts.readthedocs.io/en/latest/hadoop.html http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/ http://www.michael-noll.com/tutorials/ https://wiki.apache.org/hadoop/FrontPage . Data mining using hive: http://blog.sqlauthority.com/2013/10/21/big-data-data-mining-with-hive-what-is-hive-what-is-hiveql-hql-day-15-of-21/ DAta mining hadoop: . https://developer.yahoo.com/hadoop/tutorial/module3.html . Create a recomender with Mahout in 5 minutes: https://mahout.apache.org/users/recommender/userbased-5-minutes.html . Apache Flume for streams: http://flume.apache.org/ . ASterix DB: https://wiki.umiacs.umd.edu/ccc/images/3/32/CLuE-Li.pdf http://asterix.ics.uci.edu/ .",
            "url": "https://jpons.es/2016/06/08/big-data-lab",
            "relUrl": "/2016/06/08/big-data-lab",
            "date": " • Jun 8, 2016"
        }
        
    
  
    
        ,"post28": {
            "title": "Install java development tools >= 8 on ubuntu linux >= 14.04",
            "content": "Hi there, I’m trying to install java development tools on my ubuntu, I skim through several guides, but none of them convinced me. All links I saw just consist on adding a repository to your apt, and then install with apt from there. But I prefer to have a deep understanding. So I went to the official download page for Java 8. And I downloaded the image called linux-x64.tar.gz The release instructions are here . Once you have the file downloaded, you have to unzip it and then move to the corresponding folder. To do so: . :~/Descargas$ tar -xvf jdk-8u92-linux-x64.tar.gz $ sudo mv ~/Descargas/jdk1.8.0_92 /usr/lib/jvm/ . But then we need to update-alternatives to tell ubuntu that we want to work with that java. For the installation of a new program, you can check the man page for update-alternatives. To install java with symbolic link under /usr/bin/java: . sudo update-alternatives --install /usr/bin/java java /usr/lib/jvm/jdk1.8.0_92/bin/java 1 . To install javac: . sudo update-alternatives --install /usr/bin/javac java /usr/lib/jvm/jdk1.8.0_92/bin/javac 1 . Now you can check in console the installed versions: . java version &quot;1.8.0_92&quot; Java(TM) SE Runtime Environment (build 1.8.0_92-b14) Java HotSpot(TM) 64-Bit Server VM (build 25.92-b14, mixed mode) jpons@bugambilla:/usr/lib/jvm/jdk1.8.0_92/bin$ javac -version javac 1.8.0_92 .",
            "url": "https://jpons.es/2016/05/28/install-java-development-tools-8-on-ubuntu-linux-14-04",
            "relUrl": "/2016/05/28/install-java-development-tools-8-on-ubuntu-linux-14-04",
            "date": " • May 28, 2016"
        }
        
    
  
    
        ,"post29": {
            "title": "Compass not running with Grunt on Windows 7",
            "content": "I faced the issue that compass was not working on windows 7 64 bits and I solved by installing ruby and then the compass gem. Depending on your system, [you can download ruby here] (http://rubyinstaller.org/downloads/) Then don’t forget to set up the environment variables on windows I think you can do that directly from the installer, but also from Computer -&gt; Control Panel -&gt; Edit Environment Variables. Then on System Variables, add to the Path variable the ruby path, . Finally open a CMD terminal and write: . gem install compass . Now grunt should be working well: . grunt serve Running &quot;compass:server&quot; (compass) task directory .tmp/styles write .tmp/styles/main.css (0.096s) write .tmp/styles/main.css.map Done, without errors. .",
            "url": "https://jpons.es/2016/02/19/compass-not-running-with-grunt-on-windows-7",
            "relUrl": "/2016/02/19/compass-not-running-with-grunt-on-windows-7",
            "date": " • Feb 19, 2016"
        }
        
    
  
    
        ,"post30": {
            "title": "Choosing a graphic library for javascript",
            "content": "Choosing a graphic library for jscript: . there are several options: . raphael: . http://raphaeljs.com/ Light weight and highly compatible crossbrowsers. . paper.js: . http://paperjs.org/ Looks really powerfull . Fabric.js: . http://fabricjs.com/ . Faster than raphael and supports touch devices. Sample html with fabrik: . &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;script src=&quot;https://rawgit.com/kangax/fabric.js/master/dist/fabric.js&quot;&gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;canvas id=&quot;c&quot; width=&quot;300&quot; height=&quot;300&quot; style=&quot;border:1px solid #ccc&quot;&gt;&lt;/canvas&gt; &lt;script&gt; (function() { var canvas = new fabric.Canvas(&#39;c&#39;); })(); &lt;/script&gt; &lt;/body&gt; &lt;/html&gt; .",
            "url": "https://jpons.es/2015/09/25/javascript",
            "relUrl": "/2015/09/25/javascript",
            "date": " • Sep 25, 2015"
        }
        
    
  
    
        ,"post31": {
            "title": "Problems when using webservice from Javascript CORS",
            "content": "Hi, I’ve been fighting the whole day for the security constraint that makes your browser blocks your JS code when trying to reach an external server. Thas was solved on the standard Cross Origin Resource Sharing. (CORS for short). In my node JS server code I had to adapt the following: . //enable Cross Origin Resource Sharing app.use(function(req, res, next) { res.header(&quot;Access-Control-Allow-Origin&quot;, &quot;*&quot;); res.header(&#39;Access-Control-Allow-Methods&#39;, &#39;GET,PUT,POST,DELETE,OPTIONS&#39;); res.header(&quot;Access-Control-Allow-Headers&quot;, &quot;Origin, X-Requested-With, Content-Type, Accept&quot;); next(); }); . and on my Jquery I did: . //This is how your data looks like: var data = {&quot;answers&quot;: [ {&quot;testNo&quot;:&quot;1&quot;,&quot;answerNo&quot;:&quot;2&quot;,&quot;answerValue&quot;:&quot;answer1&quot;}, {&quot;testNo&quot;:&quot;1&quot;,&quot;answerNo&quot;:&quot;2&quot;,&quot;answerValue&quot;:&quot;answer1&quot;} ], &quot;userId&quot;:&quot;idUser&quot;}; // This is the object for the configuration: var config = {}; config.method = &quot;PUT&quot;; config.url = &quot;http://localhost:8080/api/users&quot;; config.contentType = &quot;application/json&quot;; config.data = JSON.stringify(data); config.datatype = &quot;text&quot;; $.ajax(config) .done(function(msg){ console.log(msg != null,&quot;Function called: &quot;+msg); }); . The full code of this is available at my github account .",
            "url": "https://jpons.es/2015/08/26/problems-when-using-webservice-from-javascript-cors",
            "relUrl": "/2015/08/26/problems-when-using-webservice-from-javascript-cors",
            "date": " • Aug 26, 2015"
        }
        
    
  
    
        ,"post32": {
            "title": "Using Git as SCM and exclude configuration files",
            "content": "Hi, this post is devoted to explain how do I deal with configuration files when I post them to GIT. There are very sofisticated proposals: . Git Tools - Interactive Staging | when you have secret key in your project, how can pushing to GitHub be possible? | . But the working thing to me is the following: . First push the empty config file to your repo. | Then tell git to ignore the updates on that file | Jose Enrique@MORTIMER /C/Users/Jose Enrique/Documents/nodejs_mongo/nodejs_mongo_server (master) $ git update-index --assume-unchanged config/config.js&lt;/pre&gt; .",
            "url": "https://jpons.es/2015/08/25/using-git-as-scm-and-exclude-configuration-files",
            "relUrl": "/2015/08/25/using-git-as-scm-and-exclude-configuration-files",
            "date": " • Aug 25, 2015"
        }
        
    
  
    
        ,"post33": {
            "title": "Firsts steps with MongoDB + mongoose",
            "content": "Basically I want to create a database for my JSON documents. Documents have the following format. . {&quot;userId&quot;:&quot;userIdValue&quot;, answers:[ {&quot;idUser&quot;:1111,&quot;testNo&quot;:1,&quot;answerNo&quot;:2,&quot;answerValue&quot;:&quot;answer1&quot;}, {&quot;idUser&quot;:1111,&quot;testNo&quot;:1,&quot;answerNo&quot;:2,&quot;answerValue&quot;:&quot;answer1&quot;} ]} . The steps to be able to store documents on my mongoDB are the following: . Create a collection for my docs.(It’s created implicitly) | Create a power user on that collection&lt; | Do basic CRUD operations on the database to test. i’ll create the collection “answers” and insert a test document. | . db.answers.insert( { &quot;userId&quot;:&quot;userIdValue&quot;, answers:[ {&quot;idUser&quot;:1111,&quot;testNo&quot;:1,&quot;answerNo&quot;:2,&quot;answerValue&quot;:&quot;answer1&quot;}, {&quot;idUser&quot;:1111,&quot;testNo&quot;:1,&quot;answerNo&quot;:2,&quot;answerValue&quot;:&quot;answer1&quot;} ] } ) . Configure the connection in the adapter for your app. . In my case I will configure mongoose, I’m using Node JS to connect to MongoDB. . . Watch out! As of version 3.0, the authentication mechanism by default changed So I was getting this error, mainly due to an old mongoose lib: . 2015-08-22T10:56:22.477+0200 I ACCESS [conn17] Failed to authenticate adminUser@users with mechanism MONGODB-CR: AuthenticationFailed MONGODB-CR credentials missing in the user document 2015-08-22T10:56:22.477+0200 I ACCESS [conn18] authenticate db: users { authenticate: 1, user: &quot;adminUser&quot;, nonce: &quot;xxx&quot;, key: &quot;xxx&quot; } . The solution is simple, first check the admin user has the role userAdminAnyDatabase. If you already created it: . use admin; db.updateUser( &quot;admin&quot;, {roles: [ { role: &quot;userAdminAnyDatabase&quot;, db: &quot;admin&quot; } ] } ) . update to the lastest mongoose client library, on nodeJS update your package.json: . { &quot;name&quot;: &quot;node-server&quot;, &quot;main&quot;: &quot;server.js&quot;, &quot;dependencies&quot;: { &quot;express&quot;: &quot;~4.0.0&quot;, &quot;mongoose&quot;: &quot;~4.1.3&quot;, &quot;body-parser&quot;: &quot;~1.13.3&quot; } } . Now you will see your client can authenticate: . 2015-08-22T11:31:53.914+0200 I NETWORK [initandlisten] connection accepted from 127.0.0.1:51726 #32 (2 connections now open) 2015-08-22T11:31:54.426+0200 I ACCESS [conn32] Successfully authenticated as principal adminUser on users . Now let’s test the user storage: On your nodeJS code: . var user = new User(req.body); //try to save the user: user.save(function (err){ if(err){ res.send(err); }else{ res.json({message:&quot;User created&quot;}); } }) . Notice that the user is stored on database users, on collection users: . &gt; db.users.find(); . { &quot;_id&quot; : ObjectId(&quot;55d841a60f8123581f590c58&quot;), &quot;userId&quot; : 1254, &quot;answers&quot; : [ { &quot;testNo&quot; : 1, &quot;answerNo&quot; : 1, &quot;answerValue&quot; : &quot;Myanswer1&quot;, &quot;_id&quot; : ObjectId(&quot;55d841a60f8123581f590c5e&quot;) }, { &quot;testNo&quot; : 1, &quot;answerNo&quot; : 2, &quot;answerValue&quot; : &quot;Mya nswer2&quot;, &quot;_id&quot; : ObjectId(&quot;55d841a60f8123581f590c5d&quot;) }, { &quot;testNo&quot; : 1, &quot;answerNo&quot; : 3, &quot;answerValue&quot; : &quot;Myanswer3&quot;, &quot;_id&quot; : ObjectId(&quot;55d841a60f8123581f590c5c&quot;) }, { &quot;testNo&quot; : 1, &quot;answerNo&quot; : 4, &quot;answerValue&quot; : &quot;Myanswer4&quot;, &quot;_id&quot; : Objec tId(&quot;55d841a60f8123581f590c5b&quot;) }, { &quot;testNo&quot; : 1, &quot;answerNo&quot; : 5, &quot;answerValue&quot; : &quot;Myanswer5&quot;, &quot;_id&quot; : ObjectId(&quot;55d841a60f8123581f590c5a&quot;) }, { &quot;testNo&quot; : 1,&quot;answerNo&quot; : 6, &quot;answerValue&quot; : &quot;Myanswer6&quot;, &quot;_id&quot; : ObjectId(&quot;55d841a60f8123581 f590c59&quot;) } ], &quot;__v&quot; : 0 } { &quot;_id&quot; : ObjectId(&quot;55d845a52b4672c819cc32c6&quot;), &quot;userId&quot; : 1254, &quot;answers&quot; : [ { &quot;testNo&quot; : 1, &quot;answerNo&quot; : 1, &quot;answerValue&quot; : &quot;Myanswer1&quot;, &quot;_id&quot; : ObjectId(&quot;55d845a52b4672c819cc32cc&quot;) }, { &quot;testNo&quot; : 1, &quot;answerNo&quot; : 2, &quot;answerValue&quot; : &quot;Mya nswer2&quot;, &quot;_id&quot; : ObjectId(&quot;55d845a52b4672c819cc32cb&quot;) }, { &quot;testNo&quot; : 1, &quot;answerNo&quot; : 3, &quot;answerValue&quot; : &quot;Myanswer3&quot;, &quot;_id&quot; : ObjectId(&quot;55d845a52b4672c819cc32ca&quot;) }, { &quot;testNo&quot; : 1, &quot;answerNo&quot; : 4, &quot;answerValue&quot; : &quot;Myanswer4&quot;, &quot;_id&quot; : ObjectId(&quot;55d845a52b4672c819cc32c9&quot;) }, { &quot;testNo&quot; : 1, &quot;answerNo&quot; : 5, &quot;answerValue&quot; : &quot;Myanswer5&quot;, &quot;_id&quot; : ObjectId(&quot;55d845a52b4672c819cc32c8&quot;) }, { &quot;testNo&quot; : 1,&quot;answerNo&quot; : 6, &quot;answerValue&quot; : &quot;Myanswer6&quot;, &quot;_id&quot; : ObjectId(&quot;55d845a52b4672c819cc32c7&quot;) } ], &quot;__v&quot; : 0 } . On my nodeJS server I’ve also set the output of the object, so I can see the MongoID!! . // show json request: console.log(&quot;Request: &quot;+JSON.stringify(user)); . On the console output: . Request: {&quot;userId&quot;:1254,&quot;_id&quot;:&quot;55d84608b0bef9841e7bf5f9&quot;,&quot;answers&quot;:[{&quot;testNo&quot;:1,&quot;answerNo&quot;:1,&quot;answerValue&quot;:&quot;Myanswer1&quot;,&quot;_id&quot;:&quot;55d84608b0bef9841e7bf5ff&quot;},{&quot;testNo&quot;:1,&quot;answerNo&quot;:2,&quot;answerValue&quot;:&quot;Myanswer2&quot;,&quot;_id&quot;:&quot;55d84608b0bef9841e7bf5fe&quot;},{&quot;testNo&quot;:1,&quot;answerNo&quot;:3,&quot;answerValue&quot;:&quot;Myanswer3&quot;,&quot;_id&quot;:&quot;55d84608b0bef9841e7bf5fd&quot;},{&quot;testNo&quot;:1,&quot;answerNo&quot;:4,&quot;answerValue&quot;:&quot;Myanswer4&quot;,&quot;_id&quot;:&quot;55d84608b0bef9841e7bf5fc&quot;},{&quot;testNo&quot;:1,&quot;answerNo&quot;:5,&quot;answerValue&quot;:&quot;Myanswer5&quot;,&quot;_id&quot;:&quot;55d84608b0bef9841e7bf5fb&quot;},{&quot;testNo&quot;:1,&quot;answerNo&quot;:6,&quot;answerValue&quot;:&quot;Myanswer6&quot;,&quot;_id&quot;:&quot;55d84608b0bef9841e7bf5fa&quot;}]} . So I got this _id: “_id”:”55d84608b0bef9841e7bf5f9” .",
            "url": "https://jpons.es/2015/08/24/firsts-steps-with-mongodb-mongoose",
            "relUrl": "/2015/08/24/firsts-steps-with-mongodb-mongoose",
            "date": " • Aug 24, 2015"
        }
        
    
  
    
        ,"post34": {
            "title": "Install MongoDB 3 on windows",
            "content": "Hi again, this is going to be a small configuration guide for mongo DB on windows. There are a few steps missing on the official instalation guide. To sum up the steps are the following: . Download the installer. | Install Windows hot fix on your system | After the installation, create the configuration and data folders on your system. To keep it simple, I created on the same program folder | . mkdir C: Program Files MongoDB data mkdir C: Program Files MongoDB data db . If you plan to run MongoDB server as windows service: | Open windows firewall for your network for mongoDB | Create a configuration file | . systemLog: destination: file path: &quot;C:/Program Files/MongoDB/data/mongod.log&quot; storage: dbPath: &quot;C:/Program Files/MongoDB/data/db&quot; . Install the windows service . c: Archivos de programa MongoDB Server 3.0 bin&amp;gt;mongod.exe --config &quot;C: Program Files MongoDB config mongod.cfg&quot; --install c: Archivos de programa MongoDB Server 3.0 bin&gt;net start mongoDB El servicio de MongoDB se ha iniciado correctamente. . Beware and check the logs, because even with a successful message you can get errors when trying to connect via the shell client. . Run shell client and connect . c: Archivos de programa MongoDB Server 3.0 bin&gt;mongo 2015-08-22T09:33:04.923+0200 I CONTROL Hotfix KB2731284 or later update is not installed, will zero-out data files MongoDB shell version: 3.0.5 connecting to: test 2015-08-22T09:33:05.988+0200 W NETWORK Failed to connect to 127.0.0.1:27017, re ason: errno:10061 No se puede establecer una conexión ya que el equipo de destino denegó expresamente dicha conexión. 2015-08-22T09:33:05.991+0200 E QUERY Error: couldn&#39;t connect to server 127.0.0.1:27017 (127.0.0.1), connection attempt failed at connect (src/mongo/shell/mongo.js:179:14) at (connect):1:6 at src/mongo/shell/mongo.js:179 exception: connect failed . So you get an error. That’s due to the windows firewall. Allow traffic from mongodb and you will be able to connect. . c: Archivos de programa MongoDB Server 3.0 bin&gt;mongo 2015-08-22T09:39:59.718+0200 I CONTROL Hotfix KB2731284 or later update is not installed, will zero-out data files MongoDB shell version: 3.0.5 connecting to: test Welcome to the MongoDB shell. For interactive help, type &quot;help&quot;. For more comprehensive documentation, see http://docs.mongodb.org/ Questions? Try the support group http://groups.google.com/group/mongodb-user .",
            "url": "https://jpons.es/2015/08/22/install-mongodb-3-0-on-windows",
            "relUrl": "/2015/08/22/install-mongodb-3-0-on-windows",
            "date": " • Aug 22, 2015"
        }
        
    
  
    
        ,"post35": {
            "title": "Indexing a website using javascript search engine",
            "content": "Hi, today I’m going to talk about an implementation on Javascript, called tipue search. There are 2 possibilities, one is to perform the search online, that is, to lookup the files and do the search while querying. This is reported as not being as efficient as do an offline index. . So I build an index, based on json, using a python script. . Basically the json I wanted to create looks like this: . var tipuesearch = {&quot;pages&quot;: [ {&quot;title&quot;: &quot;Welcome to JIVE - Guidelines | &quot;, &quot;text&quot;: &quot;Welcome to the Jive Collaboration platform Please read this document, which contains&quot;,&quot;tags&quot;: &quot;Welcome to JIVE - Guidelines | &quot;,&quot;url&quot;: &quot;http://10.0.82.13/DOC-1001.html&quot;}, {&quot;title&quot;: &quot;Datacenter FAQ - NGA ISO Hosting | &quot;, &quot;text&quot;: &quot;Where are the datacenters located? Do you subcontract activities? We have several datacenters around&quot;,&quot;tags&quot;: &quot;Datacenter FAQ - NGA ISO Hosting | &quot;,&quot;url&quot;: &quot;http://10.0.82.13/DOC-1002.html&quot;}, {&quot;title&quot;: &quot;Customer Information template | &quot;, &quot;text&quot;: &quot;This document will need to become the template that is used to create the information of the custome&quot;,&quot;tags&quot;: &quot;Customer Information template | &quot;,&quot;url&quot;: &quot;http://10.0.82.13/DOC-1003.html&quot;}, {&quot;title&quot;: &quot;ISO Customer List | &quot;, &quot;text&quot;: &quot;Please add customers as content is created. Please ensure you add the name alphabetically. Insert fo&quot;,&quot;tags&quot;: &quot;ISO Customer List | &quot;,&quot;url&quot;: &quot;http://10.0.82.13/DOC-1004.html&quot;}, {&quot;title&quot;: &quot;Accenture Phillipines/Singapore - APH - Hosted | &quot;, &quot;text&quot;: &quot;CustomerAccenture Philippines/Singaporealso known asAPHISO Project CodeAPHContract Start date01 June&quot;,&quot;tags&quot;: &quot;Accenture Phillipines/Singapore - APH - Hosted | &quot;,&quot;url&quot;: &quot;http://10.0.82.13/DOC-1006.html&quot;}, {&quot;title&quot;: &quot;KB: SAP logon screen hangs - Oracle | &quot;, &quot;text&quot;: &quot;KeywordsORA 257 00257 ORA-00257 Archivelog system hangs logon login screenSymptomSAP logon screen ap&quot;,&quot;tags&quot;: &quot;KB: SAP logon screen hangs - Oracle | &quot;,&quot;url&quot;: &quot;http://10.0.82.13/DOC-1007.html&quot;}, ]}; . The engine needs 3 attributes: . title: The title of the web site. | text:Some of the content of the site. | url: This is important, because on the search results, you will get a link with the url just to click on the results. | . I’m indexing some Jive documents. So my script in python to generate the json looks like this: . import urllib2.request import glob, os import codecs import sys import string from bs4 import BeautifulSoup def paquillo(): max_lenth = 100 tag_start = 11 output_file = open(&#39;D: Documentos index.json&#39;,&#39;w+&#39;) os.chdir(&quot;D: docs&quot;) for file in glob.glob(&quot;DOC-[0-9][0-9][0-9][0-9].html&quot;): print(&#39;Document: &#39;+file) f = codecs.open(file,encoding=&#39;utf-8&#39;) doc = BeautifulSoup(f.read()) if len(doc.select(&#39;.jive-rendered-content&#39;))&amp;gt;0: text = doc.select(&#39;.jive-rendered-content&#39;)[0] text_lenth = len(text.get_text()) if text_lenth&amp;gt;=max_lenth: content_text = text.get_text()[:max_lenth] elif text_lenth==0: content_text = doc.title.string else : content_text = text.get_text() else : content_text = doc.title.string if len(doc.select(&#39;.jive-icon-med .jive-icon-folder&#39;))&amp;gt;0: tags = doc.select(&#39;.jive-icon-med .jive-icon-folder&#39;)[0] tag_lenth = len(tags.get_text()) if tag_lenth &amp;gt; tag_start: tag_text = tags[:-tag_start] else : tag_text = doc.title.string else : tag_text = doc.title.string string_to_file = (&#39;{ &quot;title &quot;: &quot;&#39;+string.replace(doc.title.string,&#39; &quot;&#39;,&#39;&#39;).strip()+&#39; &quot;, &quot;text &quot;: &quot;&#39;+ string.replace(content_text,&#39; &quot;&#39;,&#39;&#39;).replace(&#39; n&#39;,&#39;&#39;).strip()+&#39; &quot;,&#39;+ &#39; &quot;tags &quot;: &quot;&#39;+string.replace(tag_text,&#39; &quot;&#39;,&#39;&#39;).strip()+&#39; &quot;,&#39;+ &#39; &quot;url &quot;: &quot;&#39; + &#39;http://10.0.82.13/&#39; + file +&#39; &quot;},&#39;).encode(&#39;utf-8&#39;)+&#39; n&#39; output_file.write(string_to_file) output_file.close() return 0 def main(): # sys.setdefaultencoding(&#39;utf-8&#39;) #reload(sys) paquillo() if __name__ == &quot;__main__&quot;: main() .",
            "url": "https://jpons.es/2015/06/15/indexing-a-website-using-javascript-search-engine",
            "relUrl": "/2015/06/15/indexing-a-website-using-javascript-search-engine",
            "date": " • Jun 15, 2015"
        }
        
    
  
    
        ,"post36": {
            "title": "Short Administration guide for passenger",
            "content": "Hi, I’m working on several projects with Phusion Passenger software. The administration is not really well documented and also is not so powerful. . There are some tools to analyze the memory usage like passenger-memory-stats. In my system the output is something like: . passenger-memory-stats Version: 4.0.59 Date : 2015-05-12 16:07:11 +0200 - Apache processes - *** WARNING: The Apache executable cannot be found. Please set the APXS2 environment variable to your &#39;apxs2&#39; executable&#39;s filename, or set the HTTPD environment variable to your &#39;httpd&#39; or &#39;apache2&#39; executable&#39;s filename. - Nginx processes - PID PPID VMSize Private Name - 20463 1 24.3 MB ? nginx: master process /usr/sbin/nginx -c /etc/ng inx/nginx.conf 26826 20463 30.5 MB ? nginx: worker process ### Processes: 2 ### Total private dirty RSS: 0.00 MB (?) Passenger processes - PID VMSize Private Name - 12284 83.5 MB 0.3 MB PassengerWatchdog 12287 117.9 MB 1.1 MB PassengerHelperAgent 12292 26.7 MB 0.5 MB PassengerLoggingAgent 12302 26.0 MB 0.2 MB PassengerWebHelper: master process /home/project 1/.passenger-enterprise/standalone/4.0.59/webhelper-1.6.2-x86_64-linux/Passenger WebHelper -c /tmp/passenger-standalone.1r1zr19/config -p /tmp/passenger-standalo ne.1r1zr19/ 15170 26.4 MB 0.6 MB PassengerWebHelper: worker process 16390 504.5 MB 230.2 MB Passenger RackApp: /home/myproject/curren 16676 438.2 MB 260.4 MB Passenger RackApp: /home/test/curren ### Processes: 95 ### Total private dirty RSS: 8074.19 MB *** WARNING: Please run this tool with sudo. Otherwise the private dirty RSS (a reliable metric for real memory usage) of processes cannot be determined. . The other tool for the administration is passenger-status: . passenger-status It appears that multiple Passenger instances are running. Please select a specific one by running: passenger-status &lt;PID&gt; The following Passenger instances are running: PID: 12494 PID: 14656 PID: 12302 PID: 15169 PID: 13833 PID: 15000 PID: 13441 PID: 12370 PID: 12896 PID: 12656 PID: 14192 PID: 14018 PID: 13672 PID: 13614 PID: 14395 PID: 12438 . Once you select the passenger process, you can dig in deeper: . passenger-status 12494 --verbose Version : 4.0.59 Date : 2015-05-12 16:15:21 +0200 Instance: 12494 -- General information -- Max pool size : 3 Processes : 1 Requests in top-level queue : 0 -- Application groups -- /homeproject/current#default: App root: /home/myproject/current Requests in queue: 0 * PID: 17563 Sessions: 0 Processed: 30 Uptime: 4h 0m 31s CPU: 0% Memory : 235M Last used: 19m 10s a URL : http://127.0.0.1:53765 Password: DTwE2g0zvvELqdDyviXrzi3FvNADc0PVc1l03TX084R localhost:~&amp;gt; curl -H &quot;X-Passenger-Connect-Password: DTwE2g0zvvELqdDyviXrzi3FvNADc0PVc1l03TX084R&quot; http://127.0.0.1:53765 . you can connect to the administration console, via curl. If you don’t get any answer, that means that your server is frozen. If you want to see a trace in the logs, you need to kill it sending one of those signals: SIGQUIT, SIGTERM. . You can find more information about this on the following sites: . https://www.phusionpassenger.com/documentation/Users%20guide%20Apache.html: On Point 9: 9. Analysis and system maintenance . URL scheme . Passenger optimization guide: .",
            "url": "https://jpons.es/2015/06/08/short-administration-guide-for-passenger",
            "relUrl": "/2015/06/08/short-administration-guide-for-passenger",
            "date": " • Jun 8, 2015"
        }
        
    
  
    
        ,"post37": {
            "title": "Get html document titles using python",
            "content": "Hi, today I’m going to show you the power of python. I’m working on windows platform, so, I use Idle environment, you can check more here. I wanted to create a script to read a lot of html files and write the title tag to a txt document, I’ll use that document to do an index later. . But, it looks like there are no standard functions to parse html files, so I found BeautifulSoup library to process html entities http://www.crummy.com/software/BeautifulSoup/bs4/doc/ I also used the following resources: Reading unicode characters: http://stackoverflow.com/questions/147741/character-reading-from-file-in-python . https://docs.python.org/2/install/ . Extracting text from html tree: http://stackoverflow.com/questions/328356/extracting-text-from-html-file-using-python . Get a list of files on a directory: http://stackoverflow.com/questions/2225564/get-a-filtered-list-of-files-in-a-directory http://stackoverflow.com/questions/3964681/find-all-files-in-directory-with-extension-txt-with-python . URLlib2 python: https://docs.python.org/2/library/urllib2.html . With that I could write the following script to get the title from each filename that contains a pattern: . #import urllib2.request import glob, os import codecs from bs4 import BeautifulSoup os.chdir(&quot;C: mydocs&quot;) for file in glob.glob(&quot;DOC-[0-9][0-9][0-9][0-9].html&quot;): f = codecs.open(file,encoding=&#39;utf-8&#39;) doc = BeautifulSoup(f.read()) print(file) print(doc.title.string) .",
            "url": "https://jpons.es/2015/06/08/scripting-with-python",
            "relUrl": "/2015/06/08/scripting-with-python",
            "date": " • Jun 8, 2015"
        }
        
    
  
    
        ,"post38": {
            "title": "interface check C#",
            "content": "#A small script to check interfaces on C# . To avoid certificate check: . &lt;system.net&gt; &lt;settings&gt; &lt;servicePointManager checkCertificateName=&quot;false&quot; checkCertificateRevocationList=&quot;false&quot; /&gt; &lt;/settings&gt; &lt;/system.net&gt; . In the application config file. . Some useful links: . Check if a file exists . | How to write to a text file . | How to download files from FTP . | How to sleep a thread . | Problems related to FTP with SSL: | Ignore SSL validation for certificates | [Configure FTP over SSL] (http://stackoverflow.com/questions/1355341/ftp-over-ssl-for-c-sharp) | [Ignore web certificates] (http://weblog.west-wind.com/posts/2011/Feb/11/HttpWebRequest-and-Ignoring-SSL-Certificate-Errors) | Library for FTPs on C# | [Enabling SSL on c#] (https://msdn.microsoft.com/en-us/library/system.net.ftpwebrequest.enablessl.aspx) | [Microsoft forums, How to accept SSL certificate of FTPS server] (https://social.msdn.microsoft.com/Forums/en-US/56a10641-4504-4f8b-8434-86156f8104be/how-to-accept-ssl-certificate-of-ftps-server?forum=netfxnetcom) | .",
            "url": "https://jpons.es/2015/06/08/interfacecheck",
            "relUrl": "/2015/06/08/interfacecheck",
            "date": " • Jun 8, 2015"
        }
        
    
  
    
        ,"post39": {
            "title": "Index database using tipuesearch",
            "content": "Optimizing search on websites with tipuesearch js . var tipuesearch = {&quot;pages&quot;: [ {&quot;title&quot;: &quot;Welcome to JIVE - Guidelines | ACME&quot;, &quot;text&quot;: &quot;Welcome to the ACME Jive Collaboration platform Please read this document, which contains&quot;,&quot;tags&quot;: &quot;Welcome to JIVE - Guidelines | ACME&quot;,&quot;url&quot;: &quot;http://10.0.82.13/DOC-1001.html&quot;}, {&quot;title&quot;: &quot;Datacenter FAQ - ACME ISO Hosting | ACME&quot;, &quot;text&quot;: &quot;Where are the datacenters located? Do you subcontract activities? We have several datacenters around&quot;,&quot;tags&quot;: &quot;Datacenter FAQ - ACME ISO Hosting | ACME&quot;,&quot;url&quot;: &quot;http://10.0.82.13/DOC-1002.html&quot;}, {&quot;title&quot;: &quot;Customer Information template | ACME&quot;, &quot;text&quot;: &quot;This document will need to become the template that is used to create the information of the custome&quot;,&quot;tags&quot;: &quot;Customer Information template | ACME&quot;,&quot;url&quot;: &quot;http://10.0.82.13/DOC-1003.html&quot;}, {&quot;title&quot;: &quot;ISO Customer List | ACME&quot;, &quot;text&quot;: &quot;Please add customers as content is created. Please ensure you add the name alphabetically. Insert fo&quot;,&quot;tags&quot;: &quot;ISO Customer List | ACME&quot;,&quot;url&quot;: &quot;http://10.0.82.13/DOC-1004.html&quot;}, {&quot;title&quot;: &quot;Accenture Phillipines/SiACMEpore - APH - Hosted | ACME&quot;, &quot;text&quot;: &quot;CustomerAccenture Philippines/SiACMEporealso known asAPHISO Project CodeAPHContract Start date01 June&quot;,&quot;tags&quot;: &quot;Accenture Phillipines/SiACMEpore - APH - Hosted | ACME&quot;,&quot;url&quot;: &quot;http://10.0.82.13/DOC-1006.html&quot;}, {&quot;title&quot;: &quot;KB: SAP logon screen hangs - Oracle | ACME&quot;, &quot;text&quot;: &quot;KeywordsORA 257 00257 ORA-00257 Archivelog system hangs logon login screenSymptomSAP logon screen ap&quot;,&quot;tags&quot;: &quot;KB: SAP logon screen hangs - Oracle | ACME&quot;,&quot;url&quot;: &quot;http://10.0.82.13/DOC-1007.html&quot;}, ]}; . Creation of the index database using python . import urllib2.request import glob, os import codecs import sys import string from bs4 import BeautifulSoup #os.chdir(&quot;D: MergedCopies_17042015 jive.ACME.com docs DOC-1008.html&quot;) def paquillo(): max_lenth = 100 tag_start = 11 output_file = open(&#39;D: Documentos index.json&#39;,&#39;w+&#39;) os.chdir(&quot;D: MergedCopies_17042015 jive.ACME.com docs&quot;) for file in glob.glob(&quot;DOC-[0-9][0-9][0-9][0-9].html&quot;): print(&#39;Document: &#39;+file) f = codecs.open(file,encoding=&#39;utf-8&#39;) doc = BeautifulSoup(f.read()) if len(doc.select(&#39;.jive-rendered-content&#39;))&gt;0: text = doc.select(&#39;.jive-rendered-content&#39;)[0] text_lenth = len(text.get_text()) if text_lenth&gt;=max_lenth: content_text = text.get_text()[:max_lenth] elif text_lenth==0: content_text = doc.title.string else : content_text = text.get_text() else : content_text = doc.title.string if len(doc.select(&#39;.jive-icon-med .jive-icon-folder&#39;))&gt;0: tags = doc.select(&#39;.jive-icon-med .jive-icon-folder&#39;)[0] tag_lenth = len(tags.get_text()) if tag_lenth &gt; tag_start: tag_text = tags[:-tag_start] else : tag_text = doc.title.string else : tag_text = doc.title.string string_to_file = (&#39;{ &quot;title &quot;: &quot;&#39;+string.replace(doc.title.string,&#39; &quot;&#39;,&#39;&#39;).strip()+&#39; &quot;, &quot;text &quot;: &quot;&#39;+ string.replace(content_text,&#39; &quot;&#39;,&#39;&#39;).replace(&#39; n&#39;,&#39;&#39;).strip()+&#39; &quot;,&#39;+ &#39; &quot;tags &quot;: &quot;&#39;+string.replace(tag_text,&#39; &quot;&#39;,&#39;&#39;).strip()+&#39; &quot;,&#39;+ &#39; &quot;url &quot;: &quot;&#39; + &#39;http://10.0.82.13/&#39; + file +&#39; &quot;},&#39;).encode(&#39;utf-8&#39;)+&#39; n&#39; output_file.write(string_to_file) output_file.close() return 0 def main(): # sys.setdefaultencoding(&#39;utf-8&#39;) #reload(sys) paquillo() if __name__ == &quot;__main__&quot;: main() . Resources . tipue search .",
            "url": "https://jpons.es/2015/06/08/index_database",
            "relUrl": "/2015/06/08/index_database",
            "date": " • Jun 8, 2015"
        }
        
    
  
    
        ,"post40": {
            "title": "More on operations with folders in windows and linux",
            "content": "Hi, recently I’ve been working on some issues dealing with folders on windows and on linux. For instance, . Replicate Folder Structure on Windows . With robocopy is really easy: . robocopy &quot;Source&quot; &quot;Target&quot; /e /xf * . For example: . robocopy &quot;C: JiveCopy&quot; . /e /xf * . Replicate folder structure on Linux . This requires a little bit of scripting, on bash it would be something like: . cd TARGET &amp;&amp; (cd SOURCE; find . -type d ! -name .) | xargs -i mkdir -p &quot;{}&quot; . For instance: . cd /interfaces/client1 &amp;&amp; (cd /interfaces/client_orig; find . -type d ! -name .) | xargs -i mkdir -p &quot;{}&quot; . Recently I had also the need to rename all folders containing a given pattern, that is done with the following script: . find . -iname &quot;itfq*&quot; -type d -execdir bash -c &#39;mv &quot;$1&quot; &quot;${1//itfq/itfp}&quot;&#39; _ {} ; . This script renames all folders that contains itfq pattern to itfp. .",
            "url": "https://jpons.es/2015/05/25/more-on-operations-with-folders-on-linux-and-windows",
            "relUrl": "/2015/05/25/more-on-operations-with-folders-on-linux-and-windows",
            "date": " • May 25, 2015"
        }
        
    
  
    
        ,"post41": {
            "title": "Working with dates in Java",
            "content": "I recovered an old post from 2012 on my previous blog. Here I translated and updated the post. . Working with dates is not an easy task in a programming language. Usually, because there is no standard (like it happens on databases) or sometimes, the standard is not implemented. On 2012 I talked that joda time was planned to be the new data standard on Java, and in fact, in 2015, Joda time is the facto standard ( as they claim on their site): . Joda-Time is the de facto standard date and time library for Java. From Java SE 8 onwards, users are asked to migrate to java.time (JSR-310). . I was interested in doing some calculations to check if a time point was inside a time interval or not. This is useful when implementing all Allen time operations. Here you will find the original paper from the ‘82 And some more friendly approaches here: . Allen’s algebra. | More Allen’s time intervals.. | . Working with dates on Joda time is really easy: . String startDate = &quot;18/09/2012&quot;; String endDate = &quot;22/09/2012&quot;; // datetime formatter, allows to read and write strings DateTimeFormatter dtf = new DateTimeFormatterBuilder(). appendDayOfMonth(2). appendLiteral(&quot;/&quot;). appendMonthOfYear(2). appendLiteral(&quot;/&quot;). appendYear(4, 4).toFormatter(); DateTime start = dtf.parseDateTime(startDate); DateTime end = dtf.parseDateTime(endDate); int months = Months.monthsBetween(start, end).getMonths(); DateTime newdate = lastRule.plusDays(28); for(int i=0;i&amp;lt;months;i++){ newdate = lastRule.plusDays(28); } Interval interval = new Interval(start,end); if(interval.contains(newdate)){ jTextArea2.setText( &quot;That day is included: &quot; + dtf.print(newdate)); }else{ jTextArea2.setText( &quot;The day is not included: &quot; + dtf.print(newdate)); } .",
            "url": "https://jpons.es/2015/05/11/working-dates-java",
            "relUrl": "/2015/05/11/working-dates-java",
            "date": " • May 11, 2015"
        }
        
    
  
    
        ,"post42": {
            "title": "Configure logging on C# project with Visual Studio",
            "content": "In this post I’m going to talk about the configuration of two loggers for Visual Studio 2013 C# project. I work with the free community edition. In this case we want one log for errors and another one for the actions that our program will be doing. . The first thing to do is to download and install the Apache log4net library. You can follow the instructions on the Apache project’s page. . There are three thing we have to do to configure the logger: . Configure the logger on the application. Usually an XML file. | Create the (singleton) classes for loggers. | Start logging in the application. | . First things first. There are several ways to configure the logger in the application. Some people prefer the configuration of the logger in the application config file. Your project’s .config file will look something like this: . &lt;!-- &lt;log4net configSource=&quot;log4net.config&quot; /&gt; --&gt; &lt;log4net&gt; &lt;appender name=&quot;FileAppender&quot; type=&quot;log4net.Appender.FileAppender&quot;&gt; &lt;file value=&quot;MyAppLog.log&quot; /&gt; &lt;appendToFile value=&quot;true&quot; /&gt; &lt;layout type=&quot;log4net.Layout.PatternLayout&quot;&gt; &lt;conversionPattern value=&quot;%date [%thread] %-5level %logger [%property{NDC}] - %message%newline&quot; /&gt; &lt;/layout&gt; &lt;/appender&gt; &lt;appender name=&quot;ConsoleAppender&quot; type=&quot;log4net.Appender.ConsoleAppender&quot; &gt; &lt;layout type=&quot;log4net.Layout.PatternLayout&quot;&gt; &lt;param name=&quot;Header&quot; value=&quot;[Header] r n&quot; /&gt; &lt;param name=&quot;Footer&quot; value=&quot;[Footer] r n&quot; /&gt; &lt;param name=&quot;ConversionPattern&quot; value=&quot;%date [%thread] %-5level %logger [%property{NDC}] - %message%newline&quot; /&gt; &lt;/layout&gt; &lt;/appender&gt; &lt;appender name=&quot;RollingFileAppender&quot; type=&quot;log4net.Appender.RollingFileAppender&quot;&gt; &lt;file value=&quot;MyAppRollingLog.log&quot; /&gt; &lt;appendToFile value=&quot;true&quot; /&gt; &lt;rollingStyle value=&quot;Size&quot; /&gt; &lt;maxSizeRollBackups value=&quot;10&quot; /&gt; &lt;maximumFileSize value=&quot;1MB&quot; /&gt; &lt;staticLogFileName value=&quot;true&quot; /&gt; &lt;layout type=&quot;log4net.Layout.PatternLayout&quot;&gt; &lt;conversionPattern value=&quot;%date [%thread] %level %logger - %message%newline&quot; /&gt; &lt;/layout&gt; &lt;/appender&gt; &lt;root&gt; &lt;level value=&quot;ALL&quot; /&gt; &lt;appender-ref ref=&quot;FileAppender&quot; /&gt; &lt;appender-ref ref=&quot;ConsoleAppender&quot;/&gt; &lt;appender-ref ref=&quot;RollingFileAppender&quot;/&gt; &lt;/root&gt; &lt;/log4net&gt; . In this sample configuration file there are configured three appenders, one for console and two for file output, the RollingFileAppender is configured to roll the log file every 1MB in a maximum of 10 extensions. I think a RollingFileAppender is the more appropriate thing for an error log in an application. Anyway I prefer the configuration of the logger in an XML file. The log4net offers the possibility to scan an XML configuration file and adapt the loggers based on that file. To do that, just write in your C# app: . static void Main(string[] args) { try { XmlConfigurator.ConfigureAndWatch(new System.IO.FileInfo(&quot;lognet_config.txt&quot;)); } catch (Exception e) { // handle the exception here } } . The other thing to do is to create a singleton class for each logger you want to use. For instance, in my app I will use 2 loggers, and the C# class is something like: . using log4net; using System; using System.Collections.Generic; using System.Linq; using System.Reflection; using System.Text; using System.Threading.Tasks; namespace MyApp.utils { class Logger { public static readonly ILog Log = LogManager.GetLogger(&quot;ErrorLog&quot;); public static readonly ILog ActionInfo = LogManager.GetLogger(&quot;ActionInfo&quot;); } } . Now we are ready to use the logger in our application, all that uses the Log logger will be written in an error log and all that uses ActionInfo log will be written to an action log. So for instance you may have this code to convert a String value into an Int32 value: . protected Int32 readIntValue(String field, Object val) { Int32 res = new Int32(); try { String str = val.ToString(); if (str.CompareTo(&quot;&quot;) != 0) res = Convert.ToInt32(str); MyApp.utils.ActionInfo.Info(&quot;Field Parsed&quot;); } catch (Exception) { MyApp.utils.Logger.Log.Debug(&quot;Error converting field:&quot; + field); } return res; } . On line 8 we will write the output to the actionInfo logger (that will be written into a rolling logger file) and the errors to the Log file, that will be written into another rolling file. Let’s take a look at the final XML configuration file: . &lt;log4net&gt; &lt;appender name=&quot;Console&quot; type=&quot;log4net.Appender.ConsoleAppender&quot;&gt; &lt;layout type=&quot;log4net.Layout.PatternLayout&quot;&gt; &lt;!-- Pattern to output the caller&#39;s file name and line number --&gt; &lt;conversionPattern value=&quot;%d %5level [%thread] (%file:%line) - %message%newline&quot; /&gt; &lt;/layout&gt; &lt;/appender&gt; &lt;appender name=&quot;ErrorLog&quot; type=&quot;log4net.Appender.RollingFileAppender&quot;&gt; &lt;file value=&quot;error.log&quot; /&gt; &lt;appendToFile value=&quot;true&quot; /&gt; &lt;maximumFileSize value=&quot;100KB&quot; /&gt; &lt;maxSizeRollBackups value=&quot;2&quot; /&gt; &lt;layout type=&quot;log4net.Layout.PatternLayout&quot;&gt; &lt;conversionPattern value=&quot;%d %level %thread %logger - %message%newline&quot; /&gt; &lt;/layout&gt; &lt;/appender&gt; &lt;appender name=&quot;ActionInfo&quot; type=&quot;log4net.Appender.RollingFileAppender&quot;&gt; &lt;file value=&quot;actionInfo.log&quot; /&gt; &lt;appendToFile value=&quot;true&quot; /&gt; &lt;maximumFileSize value=&quot;1000KB&quot; /&gt; &lt;maxSizeRollBackups value=&quot;2&quot; /&gt; &lt;layout type=&quot;log4net.Layout.PatternLayout&quot;&gt; &lt;conversionPattern value=&quot;%d %level %thread %logger - %message%newline&quot; /&gt; &lt;/layout&gt; &lt;/appender&gt; &lt;logger additivity=&quot;false&quot; name=&quot;ActionInfo&quot;&gt; &lt;level value=&quot;INFO&quot;/&gt; &lt;appender-ref ref=&quot;ActionInfo&quot; /&gt; &lt;/logger&gt; &lt;logger additivity=&quot;false&quot; name=&quot;ErrorLog&quot;&gt; &lt;level value=&quot;DEBUG&quot;/&gt; &lt;appender-ref ref=&quot;ErrorLog&quot; /&gt; &lt;/logger&gt; &lt;root&gt; &lt;appender-ref ref=&quot;Console&quot; /&gt; &lt;appender-ref ref=&quot;ErrorLog&quot; /&gt; &lt;/root&gt; &lt;/log4net&gt; .",
            "url": "https://jpons.es/2015/05/08/configure-logging-c-project-visual-studio",
            "relUrl": "/2015/05/08/configure-logging-c-project-visual-studio",
            "date": " • May 8, 2015"
        }
        
    
  
    
        ,"post43": {
            "title": "How to automate folder creation in windows",
            "content": "In this post I’m going to show some script files using the old batch scripting style that works perfectly on windows 7 an on. You will find very good books and reviews on the internet like Windows_Batch_Scripting. . In my case we need to automate the creation of the following folder structure: . Main Folder | Sub Folder | Sub Sub Folder - Folder 1 - Folder 2 - Folder 3 - Folder n | . So I developed the following a small batch script to create interfaces. First I check the arguments and set some initial variables, like the starting folder and so. . @ECHO OFF REM Create folders for a new interface REM Parameters: Folder SubFolder SubSubfolder set basefolder=c: set folders=(folder1 folder2 folder3 foldern) set argC=0 for %%x in (%*) do Set /A argC+=1 :parametercheck if -%argC%- lss 3 ( goto :wrongcall ) . The script loops on the folders variable and create the structure: . echo Creating folder interfaces: :folderloop for %%i in %folders% do ( echo mkdir %basefolder% %1 %2 %3 %%i mkdir %basefolder% %1 %2 %3 %%i ) goto :end . Finally I handle wrong calls with the following code: . :wrongcall echo Wrong call. echo %0 Folder SubFolder SubSubFolder echo Where echo Folder : Is the main folder echo SubFolder : Is the secondary folder echo SubSubFolder Is the SubSubFolder echo sample usage: echo %0 folder1 sfmt int12 . I automated the creation of folders with a call from an external batch file, so I can create multiple folders: . @ECHO OFF REM Create folders sets set argC=0 for %%x in (%*) do Set /A argC+=1 :parametercheck if -%argC%- lss 1 ( goto :wrongcall ) for /F &quot;tokens=*&quot; %%A in (%1) do CALL new_itf.bat %%A goto :end :wrongcall echo Wrong call. echo %0 config_file echo Where echo config_file: configuration file echo sample usage: echo %0 folders.txt :end . Where new_itf.bat is the name I gave to the other batch file. The aspect of the configuration file is quite simple: . Folder SubFolder SubSubFolder1 Folder SubFolder SubSubFolder2 Folder SubFolder SubSubFolder3 Folder SubFolder SubSubFolder4 .",
            "url": "https://jpons.es/2015/05/07/how-to-automate-folder-creation-and-folders-structure-creation-on-windows",
            "relUrl": "/2015/05/07/how-to-automate-folder-creation-and-folders-structure-creation-on-windows",
            "date": " • May 7, 2015"
        }
        
    
  
    
        ,"post44": {
            "title": "Automate folder creation on Windows",
            "content": "Replicate folder creation on Windows . How to automate folder creation and folders structure creation on windows: . @ECHO OFF REM Create folders for a new interface REM Parameters: GCC HRIS ITFT set basefolder=f: interfaces set folders=(in out reports staging archive) set argC=0 for %%x in (%*) do Set /A argC+=1 :parametercheck if -%argC%- lss 3 ( goto :wrongcall ) echo Creating folder interfaces: :folderloop for %%i in %folders% do ( echo mkdir %basefolder% %1 %2 %3 %%i mkdir %basefolder% %1 %2 %3 %%i ) goto :end REM echo f: interfaces %1 %2 %3 in REM mkdir f: interfaces %1 %2 %3 in REM mkdir :wrongcall echo Wrong call. echo %0 GCC HRIS ITFTxx echo Where echo GCC: Global Customer Name echo HRIS: third party name echo ITFTxx interface number. Example: itft01 echo sample usage: echo %0 abv sfsf itfq01 :end .",
            "url": "https://jpons.es/2015/05/07/create_folders-structures",
            "relUrl": "/2015/05/07/create_folders-structures",
            "date": " • May 7, 2015"
        }
        
    
  
    
        ,"post45": {
            "title": "How to create and modify admin users in Mongo DB",
            "content": "Today’s post is related to some repetitive administrative tasks related to database administration like the setup of a user and grant some permissions on Mongo db. I will perform all activities on the Javascript shell for Mongo. . First of all, it’s important to know whether mongo server is running or not. In windows, you may run it as a service. But in this case, I’ll run it in foreground: . C: Users JoseEnriqueP&gt;mongod --dbpath . 2015-05-06T23:05:03.205+0200 2015-05-06T23:05:03.222+0200 warning: 32-bit servers don&#39;t have journaling enabled by default. Please use --journal if you want durability.&lt;/pre&gt; . Connect to mongo in the usual way and start it up: . C: Users JoseEnriqueP&amp;gt;mongo MongoDB shell version: 2.6.5 connecting to: test . Change to use the administration database and create an administration user. . In this link you will see how to create an admin user in Mongo DB and also how to manage roles and user profiles in mongo. . &gt;use admin; switched to db admin db.createUser( { user: &quot;admin&quot;, pwd: &quot;yourPasswordHere&quot;, roles: [ { role: &quot;dbOwner&quot;, db: &quot;admin&quot; } ] } ) . Note that the user is now the owner of that db. You will get a confirmation that the user was created successfully: . Successfully added user: { &quot;user&quot; : &quot;admin&quot;, &quot;roles&quot; : [ { &quot;role&quot; : &quot;dbOwner&quot;, &quot;db&quot; : &quot;admin&quot; } ] } . In this link you have a complete reference for all users related operations that can be done on Mongo DB. . To change the password of a given user in Mongo DB you need an administrative user. After that, you log into the database and change the password in the following way: . &gt;db.changeUserPassword(&#39;user&#39;,&#39;newpassword&#39;); . You won’t get any notification that the password was changed, but try to log with the new password and you should get a ‘1’: . &gt;db.auth(&#39;admin&#39;,&#39;newpass&#39;); 1 .",
            "url": "https://jpons.es/2015/05/06/how-to-create-and-modify-admin-users-in-mongo-db",
            "relUrl": "/2015/05/06/how-to-create-and-modify-admin-users-in-mongo-db",
            "date": " • May 6, 2015"
        }
        
    
  
    
        ,"post46": {
            "title": "Handling Mysql permissions",
            "content": ". This is for an old version of MySQL but it may work on your current version Handling MySQL users and permissions . grant all privileges on &lt;gcc&gt;.* to &#39;&lt;gcc&gt;&#39;@&#39;localhost&#39; identified by &#39;&lt;password&#39;; grant all privileges on &lt;gcc&gt;.* to &#39;&lt;gcc&gt;&#39;@&#39;&lt;FQN_ROR_hostname&gt; &#39; identified by &#39;&lt;password&#39;; . Resources . 13.7.5.17 SHOW GRANTS Syntax .",
            "url": "https://jpons.es/2015/02/07/handle-mysql-user-permissions",
            "relUrl": "/2015/02/07/handle-mysql-user-permissions",
            "date": " • Feb 7, 2015"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". I love computers and music. And I keep on studying both. Learning never ends. . Over the past 20 years, I worked in multiple companies from small to multinationals in a wide variety of roles. From developer to systems admin to enterprise architect working for the CTO. I’m always trying to get the full picture, the end-to-end flow. And I love learning and getting a deep understanding of how something works. . I think Artificial Intelligence (AI) is already playing a strong game in the industry but there is more to come. There are many challenges, besides keeping up with the latest and greatest models and research. . As enterprise architect, I worked with the CTO team to design and implement the technology strategy and worked with multiple teams to bring AI and ML to different business areas. Introducing the cloud, migrating to the cloud, or bringing AI/ML to the cloud is one of my topics. I worked with multiple providers, including AWS, Azure, Google, and Open Telekom Cloud. I’m currently certified as Associate Architect in AWS. During my journey, I discovered that I love mentoring. I enjoy discovering potential and giving colleagues advice to grow. . In AI, there are three main areas I’m interested in: . Natural Language Processing: Understanding language and classifying or predicting from it. Most companies/businesses have their own jargon and it’s a challenge to train a model or implement any kind of knowledge transfer. | Audio processing using AI: Understanding sounds and deriving properties from them so we can classify or even predict them. | MLOps: The lifecycle of the machine learning / AI projects. The main challenges we have today are dealing with data sources, data versioning, data security, data privacy, and the implementation of an effective workflow to work with. | AIOps: Using AI for predictive maintenance, predicting failures, or detecting bugs. | . I hold a Ph.D. in Artificial Intelligence and Computer Science from the University of Granada (Spain). I studied how to model time. Using ML techniques like Fuzzy Logic and Possibility theory I propose a model to store time-related information in databases. The work was based on the previous research of Intelligent Databases and Information System (IdBIS) in the DECSAI department within the University of Granada. You can read my thesis here. I also hold a Ph.D. in Artificial Intelligence from Ghent University (Belgium). There I studied how to model queries related to time or with time constraints. I extended the bipolar queries developed in Ghent by using time. You can read my thesis here. . youtube: https://youtu.be/EuPerxeI5BQ . I’m also a violinist, and I play in a string quartet Tres Mas Uno, if you want to see us, watch our channel or follow us on instagram. . If you want to contact me, you can reach me out via linkedin or the social media. .",
          "url": "https://jpons.es/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  

  

  

  
  

  
      ,"page14": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jpons.es/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}